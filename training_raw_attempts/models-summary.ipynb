{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h_Cx9q2QFgM7"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, 3, padding=1),  # 1*3*3*4 + 4 = 40 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(4),              # 8 params\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(4, 8, 3, padding=1),  # 4*3*3*8 + 8 = 296 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),              # 16 params\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 12, 3, padding=1), # 8*3*3*12 + 12 = 876 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(12),             # 24 params\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        \n",
        "        # 12 channels * 3 * 3 = 108 neurons after three max pools (28->14->7->3)\n",
        "        self.fc1 = nn.Linear(12 * 3 * 3, 10)  # 108*10 + 10 = 1090 params\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 12 * 3 * 3)  # Flatten\n",
        "        x = F.dropout(x, p=0.1)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xdydjYTZFyi3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Net                                      [1, 10]                   --\n",
              "├─Sequential: 1-1                        [1, 4, 14, 14]            --\n",
              "│    └─Conv2d: 2-1                       [1, 4, 28, 28]            40\n",
              "│    └─ReLU: 2-2                         [1, 4, 28, 28]            --\n",
              "│    └─BatchNorm2d: 2-3                  [1, 4, 28, 28]            8\n",
              "│    └─MaxPool2d: 2-4                    [1, 4, 14, 14]            --\n",
              "├─Sequential: 1-2                        [1, 8, 7, 7]              --\n",
              "│    └─Conv2d: 2-5                       [1, 8, 14, 14]            296\n",
              "│    └─ReLU: 2-6                         [1, 8, 14, 14]            --\n",
              "│    └─BatchNorm2d: 2-7                  [1, 8, 14, 14]            16\n",
              "│    └─MaxPool2d: 2-8                    [1, 8, 7, 7]              --\n",
              "│    └─Dropout: 2-9                      [1, 8, 7, 7]              --\n",
              "├─Sequential: 1-3                        [1, 12, 3, 3]             --\n",
              "│    └─Conv2d: 2-10                      [1, 12, 7, 7]             876\n",
              "│    └─ReLU: 2-11                        [1, 12, 7, 7]             --\n",
              "│    └─BatchNorm2d: 2-12                 [1, 12, 7, 7]             24\n",
              "│    └─MaxPool2d: 2-13                   [1, 12, 3, 3]             --\n",
              "│    └─Dropout: 2-14                     [1, 12, 3, 3]             --\n",
              "├─Linear: 1-4                            [1, 10]                   1,090\n",
              "==========================================================================================\n",
              "Total params: 2,350\n",
              "Trainable params: 2,350\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.13\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.08\n",
              "Params size (MB): 0.01\n",
              "Estimated Total Size (MB): 0.10\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "# Create a dummy input tensor on the correct device\n",
        "summary(model, input_size=(1, 1, 28, 28), device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/dhairyashil/miniconda3/envs/erav2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/Users/dhairyashil/miniconda3/envs/erav2/lib/python3.12/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "torch.manual_seed(1456)\n",
        "batch_size = 512\n",
        "\n",
        "kwargs = {'num_workers': 0, 'pin_memory': True} if device.type in [\"cuda\", \"mps\"] else {}\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=True, download=True,\n",
        "#                     transform=transforms.Compose([\n",
        "#                         transforms.ToTensor(),\n",
        "#                         transforms.Normalize((0.1307,), (0.3081,))\n",
        "#                     ])),\n",
        "#     batch_size=batch_size, shuffle=True, **kwargs)\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "#                         transforms.ToTensor(),\n",
        "#                         transforms.Normalize((0.1307,), (0.3081,))\n",
        "#                     ])),\n",
        "#     batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "# Define the augmentation pipeline\n",
        "train_transforms = A.Compose([\n",
        "    A.ShiftScaleRotate(\n",
        "        shift_limit=0.0625,\n",
        "        scale_limit=0.1,\n",
        "        rotate_limit=15,\n",
        "        p=0.7,\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        value=0\n",
        "    ),\n",
        "    A.GridDistortion(num_steps=5, distort_limit=0.2, p=0.3),\n",
        "    A.GaussNoise(var_limit=(5.0, 30.0), p=0.3),\n",
        "    A.Perspective(scale=(0.05, 0.1), p=0.3, keep_size=True, pad_mode=cv2.BORDER_CONSTANT, pad_val=0),\n",
        "\n",
        "    A.ElasticTransform(\n",
        "         alpha=1.0,\n",
        "         sigma=10.0,\n",
        "         alpha_affine=None,  # Set to None as required by newer versions\n",
        "         interpolation=cv2.INTER_LINEAR,\n",
        "         border_mode=cv2.BORDER_CONSTANT,\n",
        "         value=0,\n",
        "         p=0.3\n",
        "    ),\n",
        "    \n",
        "    # CoarseDropout as alternative to regular dropout\n",
        "    A.CoarseDropout(\n",
        "        max_holes=2,\n",
        "        max_height=8,\n",
        "        max_width=8,\n",
        "        min_holes=1,\n",
        "        fill_value=0,\n",
        "        p=0.2\n",
        "    ),\n",
        "\n",
        "    A.Normalize(\n",
        "        mean=[0.1307],\n",
        "        std=[0.3081],\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# Custom Dataset class to work with Albumentations\n",
        "class MNISTAlbumentations(datasets.MNIST):\n",
        "    def __init__(self, root, train=True, download=True, transform=None):\n",
        "        super().__init__(root, train=train, download=download, transform=None)\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.data[idx], self.targets[idx]\n",
        "        \n",
        "        # Convert to numpy array and add channel dimension\n",
        "        img = np.array(img)\n",
        "        img = np.expand_dims(img, axis=-1)  # Add channel dimension for Albumentations\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            transformed = self.transform(image=img)\n",
        "            img = transformed[\"image\"]\n",
        "            \n",
        "        return img, label\n",
        "\n",
        "\n",
        "# Update the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    MNISTAlbumentations('../data', train=True, download=True, transform=train_transforms),\n",
        "    batch_size=batch_size, shuffle=True, \n",
        "    **kwargs)\n",
        "\n",
        "# Test transforms (only normalization, no augmentation)\n",
        "test_transforms = A.Compose([\n",
        "    A.Normalize(\n",
        "        mean=[0.1307],\n",
        "        std=[0.3081],\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    MNISTAlbumentations('../data', train=False, transform=test_transforms),\n",
        "    batch_size=batch_size, shuffle=True,  \n",
        "    **kwargs)\n",
        "\n",
        "# Optional: Visualization function to check augmentations\n",
        "def visualize_augmentations(dataset, idx=0, samples=5):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for i in range(samples):\n",
        "        data = dataset[idx][0]\n",
        "        if isinstance(data, torch.Tensor):\n",
        "            data = data.numpy()\n",
        "        if data.shape[0] == 1:  # If channels first, move to last\n",
        "            data = np.transpose(data, (1, 2, 0))\n",
        "        plt.subplot(1, samples, i + 1)\n",
        "        plt.imshow(data.squeeze(), cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x400 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples in train dataset: 60000\n",
            "Number of samples in test dataset: 10000\n"
          ]
        }
      ],
      "source": [
        "# Uncomment to visualize augmentations\n",
        "visualize_augmentations(train_loader.dataset)\n",
        "\n",
        "# # print number of samples in train and test dataset\n",
        "print(f\"Number of samples in train dataset: {len(train_loader.dataset)}\")\n",
        "print(f\"Number of samples in test dataset: {len(test_loader.dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch, scheduler=None):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMWbLWO6FuHb"
      },
      "outputs": [],
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "for epoch in range(1, 14):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "\n",
        "\"\"\"\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, 3, padding=1),  # 1*3*3*4 + 4 = 40 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(4),              # 8 params\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(4, 8, 3, padding=1),  # 4*3*3*8 + 8 = 296 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),              # 16 params\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 12, 3, padding=1), # 8*3*3*12 + 12 = 876 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(12),             # 24 params\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        \n",
        "        # 12 channels * 3 * 3 = 108 neurons after three max pools (28->14->7->3)\n",
        "        self.fc1 = nn.Linear(12 * 3 * 3, 10)  # 108*10 + 10 = 1090 params\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 12 * 3 * 3)  # Flatten\n",
        "        x = F.dropout(x, p=0.1)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "Net                                      [1, 10]                   --\n",
        "├─Sequential: 1-1                        [1, 4, 14, 14]            --\n",
        "│    └─Conv2d: 2-1                       [1, 4, 28, 28]            40\n",
        "│    └─ReLU: 2-2                         [1, 4, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-3                  [1, 4, 28, 28]            8\n",
        "│    └─MaxPool2d: 2-4                    [1, 4, 14, 14]            --\n",
        "├─Sequential: 1-2                        [1, 8, 7, 7]              --\n",
        "│    └─Conv2d: 2-5                       [1, 8, 14, 14]            296\n",
        "│    └─ReLU: 2-6                         [1, 8, 14, 14]            --\n",
        "│    └─BatchNorm2d: 2-7                  [1, 8, 14, 14]            16\n",
        "│    └─MaxPool2d: 2-8                    [1, 8, 7, 7]              --\n",
        "│    └─Dropout: 2-9                      [1, 8, 7, 7]              --\n",
        "├─Sequential: 1-3                        [1, 12, 3, 3]             --\n",
        "│    └─Conv2d: 2-10                      [1, 12, 7, 7]             876\n",
        "│    └─ReLU: 2-11                        [1, 12, 7, 7]             --\n",
        "│    └─BatchNorm2d: 2-12                 [1, 12, 7, 7]             24\n",
        "│    └─MaxPool2d: 2-13                   [1, 12, 3, 3]             --\n",
        "│    └─Dropout: 2-14                     [1, 12, 3, 3]             --\n",
        "├─Linear: 1-4                            [1, 10]                   1,090\n",
        "==========================================================================================\n",
        "Total params: 2,350\n",
        "Trainable params: 2,350\n",
        "Non-trainable params: 0\n",
        "Total mult-adds (Units.MEGABYTES): 0.13\n",
        "==========================================================================================\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 0.08\n",
        "Params size (MB): 0.01\n",
        "Estimated Total Size (MB): 0.10\n",
        "==========================================================================================\n",
        "\n",
        "\n",
        "loss=0.46857449412345886 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.72it/s]\n",
        "\n",
        "Test set: Average loss: 0.1879, Accuracy: 9446/10000 (94.46%)\n",
        "\n",
        "loss=0.3038197457790375 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.82it/s] \n",
        "\n",
        "Test set: Average loss: 0.1234, Accuracy: 9638/10000 (96.38%)\n",
        "\n",
        "loss=0.37657299637794495 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.80it/s]\n",
        "\n",
        "Test set: Average loss: 0.1038, Accuracy: 9687/10000 (96.87%)\n",
        "\n",
        "loss=0.46292558312416077 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.86it/s]\n",
        "\n",
        "Test set: Average loss: 0.0889, Accuracy: 9728/10000 (97.28%)\n",
        "\n",
        "loss=0.33317264914512634 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.80it/s]\n",
        "\n",
        "Test set: Average loss: 0.0854, Accuracy: 9733/10000 (97.33%)\n",
        "\n",
        "loss=0.29084160923957825 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.76it/s]\n",
        "\n",
        "Test set: Average loss: 0.0799, Accuracy: 9766/10000 (97.66%)\n",
        "\n",
        "loss=0.41878804564476013 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.77it/s]\n",
        "\n",
        "Test set: Average loss: 0.0801, Accuracy: 9744/10000 (97.44%)\n",
        "\n",
        "loss=0.2115659862756729 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.54it/s] \n",
        "\n",
        "Test set: Average loss: 0.0788, Accuracy: 9770/10000 (97.70%)\n",
        "\n",
        "loss=0.3731565475463867 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.66it/s] \n",
        "\n",
        "Test set: Average loss: 0.0752, Accuracy: 9756/10000 (97.56%)\n",
        "\n",
        "loss=0.2608632743358612 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.77it/s] \n",
        "\n",
        "Test set: Average loss: 0.0703, Accuracy: 9779/10000 (97.79%)\n",
        "\n",
        "loss=0.24264895915985107 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.73it/s]\n",
        "\n",
        "Test set: Average loss: 0.0666, Accuracy: 9809/10000 (98.09%)\n",
        "\n",
        "loss=0.2863992750644684 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.80it/s] \n",
        "\n",
        "Test set: Average loss: 0.0754, Accuracy: 9760/10000 (97.60%)\n",
        "\n",
        "loss=0.22249417006969452 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.71it/s]\n",
        "\n",
        "Test set: Average loss: 0.0639, Accuracy: 9808/10000 (98.08%)\n",
        "\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2 ----- Second Model - Deeper Network:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, 3, padding=1),    # 40 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(4),                # 8 params\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(4, 8, 3, padding=1),    # 296 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),                # 16 params\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 12, 3, padding=1),   # 876 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(12),               # 24 params\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        \n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(12, 16, 1),             # 192 + 16 = 208 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),               # 32 params\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(16 * 3 * 3, 10)  # 1450 params\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.dropout(x, p=0.1)\n",
        "        x = self.conv3(x)\n",
        "        x = F.dropout(x, p=0.1)\n",
        "        x = self.conv4(x)\n",
        "        x = x.view(-1, 16 * 3 * 3)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "        \n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "Net                                      [1, 10]                   --\n",
        "├─Sequential: 1-1                        [1, 4, 28, 28]            --\n",
        "│    └─Conv2d: 2-1                       [1, 4, 28, 28]            40\n",
        "│    └─ReLU: 2-2                         [1, 4, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-3                  [1, 4, 28, 28]            8\n",
        "├─Sequential: 1-2                        [1, 8, 14, 14]            --\n",
        "│    └─Conv2d: 2-4                       [1, 8, 28, 28]            296\n",
        "│    └─ReLU: 2-5                         [1, 8, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-6                  [1, 8, 28, 28]            16\n",
        "│    └─MaxPool2d: 2-7                    [1, 8, 14, 14]            --\n",
        "├─Sequential: 1-3                        [1, 12, 7, 7]             --\n",
        "│    └─Conv2d: 2-8                       [1, 12, 14, 14]           876\n",
        "│    └─ReLU: 2-9                         [1, 12, 14, 14]           --\n",
        "│    └─BatchNorm2d: 2-10                 [1, 12, 14, 14]           24\n",
        "│    └─MaxPool2d: 2-11                   [1, 12, 7, 7]             --\n",
        "├─Sequential: 1-4                        [1, 16, 3, 3]             --\n",
        "│    └─Conv2d: 2-12                      [1, 16, 7, 7]             208\n",
        "│    └─ReLU: 2-13                        [1, 16, 7, 7]             --\n",
        "│    └─BatchNorm2d: 2-14                 [1, 16, 7, 7]             32\n",
        "│    └─MaxPool2d: 2-15                   [1, 16, 3, 3]             --\n",
        "├─Linear: 1-5                            [1, 10]                   1,450\n",
        "==========================================================================================\n",
        "Total params: 2,950\n",
        "Trainable params: 2,950\n",
        "Non-trainable params: 0\n",
        "Total mult-adds (Units.MEGABYTES): 0.45\n",
        "==========================================================================================\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 0.20\n",
        "Params size (MB): 0.01\n",
        "Estimated Total Size (MB): 0.22\n",
        "==========================================================================================\n",
        "\n",
        "loss=0.43610844016075134 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.47it/s]\n",
        "\n",
        "Test set: Average loss: 0.2068, Accuracy: 9376/10000 (93.76%)\n",
        "\n",
        "loss=0.49127259850502014 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.65it/s]\n",
        "\n",
        "Test set: Average loss: 0.1427, Accuracy: 9592/10000 (95.92%)\n",
        "\n",
        "loss=0.38016626238822937 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.60it/s]\n",
        "\n",
        "Test set: Average loss: 0.1205, Accuracy: 9653/10000 (96.53%)\n",
        "\n",
        "loss=0.3631477355957031 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.62it/s] \n",
        "\n",
        "Test set: Average loss: 0.1044, Accuracy: 9692/10000 (96.92%)\n",
        "\n",
        "loss=0.30866920948028564 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.63it/s]\n",
        "\n",
        "Test set: Average loss: 0.0924, Accuracy: 9716/10000 (97.16%)\n",
        "\n",
        "loss=0.16083014011383057 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.54it/s]\n",
        "\n",
        "Test set: Average loss: 0.0883, Accuracy: 9726/10000 (97.26%)\n",
        "\n",
        "loss=0.1785837858915329 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.63it/s] \n",
        "\n",
        "Test set: Average loss: 0.0850, Accuracy: 9752/10000 (97.52%)\n",
        "\n",
        "loss=0.2444532960653305 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.58it/s] \n",
        "\n",
        "Test set: Average loss: 0.0827, Accuracy: 9743/10000 (97.43%)\n",
        "\n",
        "loss=0.25175610184669495 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.62it/s]\n",
        "\n",
        "Test set: Average loss: 0.0789, Accuracy: 9761/10000 (97.61%)\n",
        "\n",
        "loss=0.17259764671325684 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.66it/s]\n",
        "\n",
        "Test set: Average loss: 0.0811, Accuracy: 9736/10000 (97.36%)\n",
        "\n",
        "loss=0.147956982254982 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.53it/s]  \n",
        "\n",
        "Test set: Average loss: 0.0747, Accuracy: 9765/10000 (97.65%)\n",
        "\n",
        "loss=0.13799484074115753 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.57it/s]\n",
        "\n",
        "Test set: Average loss: 0.0785, Accuracy: 9765/10000 (97.65%)\n",
        "\n",
        "loss=0.1758977621793747 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.58it/s] \n",
        "\n",
        "Test set: Average loss: 0.0865, Accuracy: 9728/10000 (97.28%)\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Third Model - Dilation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, padding=1, dilation=1),  # 80 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),                          # 16 params\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(8, 12, 3, padding=2, dilation=2), # 876 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(12),                         # 24 params\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(12, 16, 1),                       # 208 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),                         # 32 params\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(16 * 3 * 3, 10)           # 1450 params\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 16 * 3 * 3)\n",
        "        x = F.dropout(x, p=0.1)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "Net                                      [1, 10]                   --\n",
        "├─Sequential: 1-1                        [1, 8, 14, 14]            --\n",
        "│    └─Conv2d: 2-1                       [1, 8, 28, 28]            80\n",
        "│    └─ReLU: 2-2                         [1, 8, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-3                  [1, 8, 28, 28]            16\n",
        "│    └─MaxPool2d: 2-4                    [1, 8, 14, 14]            --\n",
        "├─Sequential: 1-2                        [1, 12, 7, 7]             --\n",
        "│    └─Conv2d: 2-5                       [1, 12, 14, 14]           876\n",
        "│    └─ReLU: 2-6                         [1, 12, 14, 14]           --\n",
        "│    └─BatchNorm2d: 2-7                  [1, 12, 14, 14]           24\n",
        "│    └─MaxPool2d: 2-8                    [1, 12, 7, 7]             --\n",
        "│    └─Dropout: 2-9                      [1, 12, 7, 7]             --\n",
        "├─Sequential: 1-3                        [1, 16, 3, 3]             --\n",
        "│    └─Conv2d: 2-10                      [1, 16, 7, 7]             208\n",
        "│    └─ReLU: 2-11                        [1, 16, 7, 7]             --\n",
        "│    └─BatchNorm2d: 2-12                 [1, 16, 7, 7]             32\n",
        "│    └─MaxPool2d: 2-13                   [1, 16, 3, 3]             --\n",
        "├─Linear: 1-4                            [1, 10]                   1,450\n",
        "==========================================================================================\n",
        "Total params: 2,686\n",
        "Trainable params: 2,686\n",
        "Non-trainable params: 0\n",
        "Total mult-adds (Units.MEGABYTES): 0.25\n",
        "==========================================================================================\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 0.15\n",
        "Params size (MB): 0.01\n",
        "Estimated Total Size (MB): 0.16\n",
        "==========================================================================================\n",
        "\n",
        "loss=0.37902402877807617 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.60it/s]\n",
        "\n",
        "Test set: Average loss: 0.1768, Accuracy: 9462/10000 (94.62%)\n",
        "\n",
        "loss=0.44349679350852966 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.76it/s]\n",
        "\n",
        "Test set: Average loss: 0.1390, Accuracy: 9571/10000 (95.71%)\n",
        "\n",
        "loss=0.3231264650821686 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.74it/s] \n",
        "\n",
        "Test set: Average loss: 0.1029, Accuracy: 9694/10000 (96.94%)\n",
        "\n",
        "loss=0.25976741313934326 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.80it/s]\n",
        "\n",
        "Test set: Average loss: 0.0965, Accuracy: 9713/10000 (97.13%)\n",
        "\n",
        "loss=0.2346165031194687 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.76it/s] \n",
        "\n",
        "Test set: Average loss: 0.0928, Accuracy: 9719/10000 (97.19%)\n",
        "\n",
        "loss=0.3042391538619995 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.71it/s] \n",
        "\n",
        "Test set: Average loss: 0.0782, Accuracy: 9762/10000 (97.62%)\n",
        "\n",
        "loss=0.1149132028222084 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.71it/s] \n",
        "\n",
        "Test set: Average loss: 0.0878, Accuracy: 9715/10000 (97.15%)\n",
        "\n",
        "loss=0.23651307821273804 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.79it/s]\n",
        "\n",
        "Test set: Average loss: 0.0741, Accuracy: 9783/10000 (97.83%)\n",
        "\n",
        "loss=0.20037126541137695 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.69it/s]\n",
        "\n",
        "Test set: Average loss: 0.0708, Accuracy: 9780/10000 (97.80%)\n",
        "\n",
        "loss=0.3102586269378662 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.79it/s] \n",
        "\n",
        "Test set: Average loss: 0.0724, Accuracy: 9766/10000 (97.66%)\n",
        "\n",
        "loss=0.2528785765171051 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.78it/s] \n",
        "\n",
        "Test set: Average loss: 0.0705, Accuracy: 9764/10000 (97.64%)\n",
        "\n",
        "loss=0.2449759989976883 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.82it/s] \n",
        "\n",
        "Test set: Average loss: 0.0622, Accuracy: 9799/10000 (97.99%)\n",
        "\n",
        "loss=0.13509982824325562 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.79it/s]\n",
        "\n",
        "Test set: Average loss: 0.0653, Accuracy: 9785/10000 (97.85%)\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### --- 4 Fourth Model - Residual Connections:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, padding=1),    # 80 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),                # 16 params\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        \n",
        "        # Main path\n",
        "        self.conv2_main = nn.Sequential(\n",
        "            nn.Conv2d(8, 12, 3, padding=1),   # 876 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(12),               # 24 params\n",
        "        )\n",
        "        \n",
        "        # Skip connection path\n",
        "        self.conv2_skip = nn.Conv2d(8, 12, 1) # 108 params\n",
        "        \n",
        "        self.pool2 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(12, 16, 3, padding=1),  # 1744 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),               # 32 params\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(16 * 3 * 3, 10)  # 1450 params\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        \n",
        "        # Residual connection\n",
        "        identity = self.conv2_skip(x)\n",
        "        x = self.conv2_main(x)\n",
        "        x = x + identity\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 16 * 3 * 3)\n",
        "        x = F.dropout(x, p=0.1)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "Net                                      [1, 10]                   --\n",
        "├─Sequential: 1-1                        [1, 8, 14, 14]            --\n",
        "│    └─Conv2d: 2-1                       [1, 8, 28, 28]            80\n",
        "│    └─ReLU: 2-2                         [1, 8, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-3                  [1, 8, 28, 28]            16\n",
        "│    └─MaxPool2d: 2-4                    [1, 8, 14, 14]            --\n",
        "├─Conv2d: 1-2                            [1, 12, 14, 14]           108\n",
        "├─Sequential: 1-3                        [1, 12, 14, 14]           --\n",
        "│    └─Conv2d: 2-5                       [1, 12, 14, 14]           876\n",
        "│    └─ReLU: 2-6                         [1, 12, 14, 14]           --\n",
        "│    └─BatchNorm2d: 2-7                  [1, 12, 14, 14]           24\n",
        "├─Sequential: 1-4                        [1, 12, 7, 7]             --\n",
        "│    └─ReLU: 2-8                         [1, 12, 14, 14]           --\n",
        "│    └─MaxPool2d: 2-9                    [1, 12, 7, 7]             --\n",
        "│    └─Dropout: 2-10                     [1, 12, 7, 7]             --\n",
        "├─Sequential: 1-5                        [1, 16, 3, 3]             --\n",
        "│    └─Conv2d: 2-11                      [1, 16, 7, 7]             1,744\n",
        "│    └─ReLU: 2-12                        [1, 16, 7, 7]             --\n",
        "│    └─BatchNorm2d: 2-13                 [1, 16, 7, 7]             32\n",
        "│    └─MaxPool2d: 2-14                   [1, 16, 3, 3]             --\n",
        "├─Linear: 1-6                            [1, 10]                   1,450\n",
        "==========================================================================================\n",
        "Total params: 4,330\n",
        "Trainable params: 4,330\n",
        "Non-trainable params: 0\n",
        "Total mult-adds (Units.MEGABYTES): 0.34\n",
        "==========================================================================================\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 0.17\n",
        "Params size (MB): 0.02\n",
        "Estimated Total Size (MB): 0.19\n",
        "==========================================================================================\n",
        "\n",
        "\n",
        "loss=0.2911372482776642 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.51it/s] \n",
        "\n",
        "Test set: Average loss: 0.1394, Accuracy: 9614/10000 (96.14%)\n",
        "\n",
        "loss=0.24787931144237518 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.79it/s]\n",
        "\n",
        "Test set: Average loss: 0.0912, Accuracy: 9725/10000 (97.25%)\n",
        "\n",
        "loss=0.3472770154476166 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.70it/s] \n",
        "\n",
        "Test set: Average loss: 0.0811, Accuracy: 9746/10000 (97.46%)\n",
        "\n",
        "loss=0.1477222740650177 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.75it/s] \n",
        "\n",
        "Test set: Average loss: 0.0818, Accuracy: 9753/10000 (97.53%)\n",
        "\n",
        "loss=0.19079406559467316 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.68it/s]\n",
        "\n",
        "Test set: Average loss: 0.0615, Accuracy: 9800/10000 (98.00%)\n",
        "\n",
        "loss=0.32724902033805847 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.70it/s]\n",
        "\n",
        "Test set: Average loss: 0.0609, Accuracy: 9815/10000 (98.15%)\n",
        "\n",
        "loss=0.21343994140625 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.75it/s]   \n",
        "\n",
        "Test set: Average loss: 0.0625, Accuracy: 9795/10000 (97.95%)\n",
        "\n",
        "loss=0.30540111660957336 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.57it/s]\n",
        "\n",
        "Test set: Average loss: 0.0581, Accuracy: 9812/10000 (98.12%)\n",
        "\n",
        "loss=0.14111433923244476 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.65it/s]\n",
        "\n",
        "Test set: Average loss: 0.0587, Accuracy: 9811/10000 (98.11%)\n",
        "\n",
        "loss=0.1231885775923729 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.76it/s] \n",
        "\n",
        "Test set: Average loss: 0.0486, Accuracy: 9841/10000 (98.41%)\n",
        "\n",
        "loss=0.28728026151657104 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.70it/s]\n",
        "\n",
        "Test set: Average loss: 0.0498, Accuracy: 9842/10000 (98.42%)\n",
        "\n",
        "loss=0.14757050573825836 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.80it/s]\n",
        "\n",
        "Test set: Average loss: 0.0517, Accuracy: 9834/10000 (98.34%)\n",
        "\n",
        "loss=0.21163912117481232 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.75it/s]\n",
        "\n",
        "Test set: Average loss: 0.0504, Accuracy: 9840/10000 (98.40%)\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, padding=1),    # 80 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),                # 16 params\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        \n",
        "        # Main path\n",
        "        self.conv2_main = nn.Sequential(\n",
        "            nn.Conv2d(8, 12, 3, padding=1),   # 876 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(12),               # 24 params\n",
        "        )\n",
        "        \n",
        "        # Skip connection path\n",
        "        self.conv2_skip = nn.Conv2d(8, 12, 1) # 108 params\n",
        "        \n",
        "        self.pool2 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(12, 8, 3, padding=1),   # 872 params (reduced from 1744)\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),                # 16 params (reduced from 32)\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(8 * 3 * 3, 10)   # 730 params (reduced from 1450)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        \n",
        "        # Residual connection\n",
        "        identity = self.conv2_skip(x)\n",
        "        x = self.conv2_main(x)\n",
        "        x = x + identity\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 8 * 3 * 3)\n",
        "        x = F.dropout(x, p=0.1)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "Net                                      [1, 10]                   --\n",
        "├─Sequential: 1-1                        [1, 8, 14, 14]            --\n",
        "│    └─Conv2d: 2-1                       [1, 8, 28, 28]            80\n",
        "│    └─ReLU: 2-2                         [1, 8, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-3                  [1, 8, 28, 28]            16\n",
        "│    └─MaxPool2d: 2-4                    [1, 8, 14, 14]            --\n",
        "├─Conv2d: 1-2                            [1, 12, 14, 14]           108\n",
        "├─Sequential: 1-3                        [1, 12, 14, 14]           --\n",
        "│    └─Conv2d: 2-5                       [1, 12, 14, 14]           876\n",
        "│    └─ReLU: 2-6                         [1, 12, 14, 14]           --\n",
        "│    └─BatchNorm2d: 2-7                  [1, 12, 14, 14]           24\n",
        "├─Sequential: 1-4                        [1, 12, 7, 7]             --\n",
        "│    └─ReLU: 2-8                         [1, 12, 14, 14]           --\n",
        "│    └─MaxPool2d: 2-9                    [1, 12, 7, 7]             --\n",
        "│    └─Dropout: 2-10                     [1, 12, 7, 7]             --\n",
        "├─Sequential: 1-5                        [1, 8, 3, 3]              --\n",
        "│    └─Conv2d: 2-11                      [1, 8, 7, 7]              872\n",
        "│    └─ReLU: 2-12                        [1, 8, 7, 7]              --\n",
        "│    └─BatchNorm2d: 2-13                 [1, 8, 7, 7]              16\n",
        "│    └─MaxPool2d: 2-14                   [1, 8, 3, 3]              --\n",
        "├─Linear: 1-6                            [1, 10]                   730\n",
        "==========================================================================================\n",
        "Total params: 2,722\n",
        "Trainable params: 2,722\n",
        "Non-trainable params: 0\n",
        "Total mult-adds (Units.MEGABYTES): 0.30\n",
        "==========================================================================================\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 0.16\n",
        "Params size (MB): 0.01\n",
        "Estimated Total Size (MB): 0.18\n",
        "==========================================================================================\n",
        "\n",
        "loss=0.35204020142555237 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.48it/s]\n",
        "\n",
        "Test set: Average loss: 0.2059, Accuracy: 9388/10000 (93.88%)\n",
        "\n",
        "loss=0.5275420546531677 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.71it/s] \n",
        "\n",
        "Test set: Average loss: 0.1152, Accuracy: 9658/10000 (96.58%)\n",
        "\n",
        "loss=0.26452550292015076 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.67it/s]\n",
        "\n",
        "Test set: Average loss: 0.0903, Accuracy: 9715/10000 (97.15%)\n",
        "\n",
        "loss=0.41384220123291016 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.62it/s]\n",
        "\n",
        "Test set: Average loss: 0.0926, Accuracy: 9722/10000 (97.22%)\n",
        "\n",
        "loss=0.15689626336097717 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.71it/s]\n",
        "\n",
        "Test set: Average loss: 0.0845, Accuracy: 9734/10000 (97.34%)\n",
        "\n",
        "loss=0.14239619672298431 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.71it/s]\n",
        "\n",
        "Test set: Average loss: 0.1022, Accuracy: 9677/10000 (96.77%)\n",
        "\n",
        "loss=0.15626820921897888 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.75it/s]\n",
        "\n",
        "Test set: Average loss: 0.0792, Accuracy: 9759/10000 (97.59%)\n",
        "\n",
        "loss=0.4029049873352051 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.70it/s] \n",
        "\n",
        "Test set: Average loss: 0.0795, Accuracy: 9748/10000 (97.48%)\n",
        "\n",
        "loss=0.11204349249601364 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.71it/s]\n",
        "\n",
        "Test set: Average loss: 0.0749, Accuracy: 9770/10000 (97.70%)\n",
        "\n",
        "loss=0.14662520587444305 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.67it/s]\n",
        "\n",
        "Test set: Average loss: 0.0790, Accuracy: 9751/10000 (97.51%)\n",
        "\n",
        "loss=0.15030880272388458 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.50it/s]\n",
        "\n",
        "Test set: Average loss: 0.0826, Accuracy: 9741/10000 (97.41%)\n",
        "\n",
        "loss=0.1625288724899292 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.65it/s] \n",
        "\n",
        "Test set: Average loss: 0.0663, Accuracy: 9790/10000 (97.90%)\n",
        "\n",
        "loss=0.17114275693893433 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.65it/s]\n",
        "\n",
        "Test set: Average loss: 0.0711, Accuracy: 9773/10000 (97.73%)\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, 3, padding=1),  # 36 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(4),              # 8 params\n",
        "            nn.Conv2d(4, 8, 3, padding=1),  # 288 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),              # 16 params\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(8, 10, 3, padding=1),  # 864 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(10),              # 24 params\n",
        "            nn.Conv2d(10, 10, 3, padding=1), # 1,296 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(10),              # 24 params\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(10, 10, 3, padding=1),  # 1,080 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(10),               # 20 params\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "Net                                      [1, 10]                   --\n",
        "├─Sequential: 1-1                        [1, 8, 14, 14]            --\n",
        "│    └─Conv2d: 2-1                       [1, 4, 28, 28]            40\n",
        "│    └─ReLU: 2-2                         [1, 4, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-3                  [1, 4, 28, 28]            8\n",
        "│    └─Conv2d: 2-4                       [1, 8, 28, 28]            296\n",
        "│    └─ReLU: 2-5                         [1, 8, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-6                  [1, 8, 28, 28]            16\n",
        "│    └─MaxPool2d: 2-7                    [1, 8, 14, 14]            --\n",
        "├─Sequential: 1-2                        [1, 10, 7, 7]             --\n",
        "│    └─Conv2d: 2-8                       [1, 10, 14, 14]           730\n",
        "│    └─ReLU: 2-9                         [1, 10, 14, 14]           --\n",
        "│    └─BatchNorm2d: 2-10                 [1, 10, 14, 14]           20\n",
        "│    └─Conv2d: 2-11                      [1, 10, 14, 14]           910\n",
        "│    └─ReLU: 2-12                        [1, 10, 14, 14]           --\n",
        "│    └─BatchNorm2d: 2-13                 [1, 10, 14, 14]           20\n",
        "│    └─MaxPool2d: 2-14                   [1, 10, 7, 7]             --\n",
        "│    └─Dropout: 2-15                     [1, 10, 7, 7]             --\n",
        "├─Sequential: 1-3                        [1, 10, 1, 1]             --\n",
        "│    └─Conv2d: 2-16                      [1, 10, 7, 7]             910\n",
        "│    └─ReLU: 2-17                        [1, 10, 7, 7]             --\n",
        "│    └─BatchNorm2d: 2-18                 [1, 10, 7, 7]             20\n",
        "│    └─AdaptiveAvgPool2d: 2-19           [1, 10, 1, 1]             --\n",
        "==========================================================================================\n",
        "Total params: 2,970\n",
        "Trainable params: 2,970\n",
        "Non-trainable params: 0\n",
        "Total mult-adds (Units.MEGABYTES): 0.63\n",
        "==========================================================================================\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 0.22\n",
        "Params size (MB): 0.01\n",
        "Estimated Total Size (MB): 0.24\n",
        "==========================================================================================\n",
        "\n",
        "  0%|          | 0/118 [00:00<?, ?it/s]loss=1.3644795417785645 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.50it/s]\n",
        "\n",
        "Test set: Average loss: 1.1745, Accuracy: 8130/10000 (81.30%)\n",
        "\n",
        "loss=0.822507917881012 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.71it/s] \n",
        "\n",
        "Test set: Average loss: 0.6233, Accuracy: 9130/10000 (91.30%)\n",
        "\n",
        "loss=0.6974770426750183 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.60it/s]\n",
        "\n",
        "Test set: Average loss: 0.4175, Accuracy: 9346/10000 (93.46%)\n",
        "\n",
        "loss=0.5282017588615417 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.64it/s] \n",
        "\n",
        "Test set: Average loss: 0.3047, Accuracy: 9485/10000 (94.85%)\n",
        "\n",
        "loss=0.4999934434890747 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.56it/s] \n",
        "\n",
        "Test set: Average loss: 0.2446, Accuracy: 9548/10000 (95.48%)\n",
        "\n",
        "loss=0.4264257848262787 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.67it/s] \n",
        "\n",
        "Test set: Average loss: 0.2154, Accuracy: 9577/10000 (95.77%)\n",
        "\n",
        "loss=0.32045114040374756 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.67it/s]\n",
        "\n",
        "Test set: Average loss: 0.1944, Accuracy: 9596/10000 (95.96%)\n",
        "\n",
        "loss=0.37733539938926697 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.58it/s]\n",
        "\n",
        "Test set: Average loss: 0.1659, Accuracy: 9667/10000 (96.67%)\n",
        "\n",
        "loss=0.3017043173313141 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.65it/s] \n",
        "\n",
        "Test set: Average loss: 0.1725, Accuracy: 9611/10000 (96.11%)\n",
        "\n",
        "loss=0.2630317211151123 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.69it/s] \n",
        "\n",
        "Test set: Average loss: 0.1529, Accuracy: 9671/10000 (96.71%)\n",
        "\n",
        "loss=0.19612835347652435 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.60it/s]\n",
        "\n",
        "Test set: Average loss: 0.1800, Accuracy: 9563/10000 (95.63%)\n",
        "\n",
        "loss=0.30790337920188904 batch_id=49:  42%|████▏     | 50/118 [00:09<00:12,  5.42it/s]\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Adam Cycles 20 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Grouped Convolution Model\n",
        "class Net(nn.Module): # Net_GroupedConv\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, kernel_size=3, padding=1, groups=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.Conv2d(4, 4, kernel_size=3, padding=1, groups=2, bias=False),  # Grouped convolution\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(4, 8, kernel_size=3, padding=1, groups=2, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.Conv2d(8, 8, kernel_size=3, padding=1, groups=4, bias=False),  # Increased groups\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 12, kernel_size=3, padding=1, groups=4, bias=False),  # Changed groups from 3 to 4\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(12 * 3 * 3, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 12 * 3 * 3)\n",
        "        x = F.dropout(x, p=0.0)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "    \n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "Net                                      [1, 10]                   --\n",
        "├─Sequential: 1-1                        [1, 4, 14, 14]            --\n",
        "│    └─Conv2d: 2-1                       [1, 4, 28, 28]            36\n",
        "│    └─ReLU: 2-2                         [1, 4, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-3                  [1, 4, 28, 28]            8\n",
        "│    └─Conv2d: 2-4                       [1, 4, 28, 28]            72\n",
        "│    └─ReLU: 2-5                         [1, 4, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-6                  [1, 4, 28, 28]            8\n",
        "│    └─MaxPool2d: 2-7                    [1, 4, 14, 14]            --\n",
        "│    └─Dropout: 2-8                      [1, 4, 14, 14]            --\n",
        "├─Sequential: 1-2                        [1, 8, 7, 7]              --\n",
        "│    └─Conv2d: 2-9                       [1, 8, 14, 14]            144\n",
        "│    └─ReLU: 2-10                        [1, 8, 14, 14]            --\n",
        "│    └─BatchNorm2d: 2-11                 [1, 8, 14, 14]            16\n",
        "│    └─Conv2d: 2-12                      [1, 8, 14, 14]            144\n",
        "│    └─ReLU: 2-13                        [1, 8, 14, 14]            --\n",
        "│    └─BatchNorm2d: 2-14                 [1, 8, 14, 14]            16\n",
        "│    └─MaxPool2d: 2-15                   [1, 8, 7, 7]              --\n",
        "│    └─Dropout: 2-16                     [1, 8, 7, 7]              --\n",
        "├─Sequential: 1-3                        [1, 12, 3, 3]             --\n",
        "│    └─Conv2d: 2-17                      [1, 12, 7, 7]             216\n",
        "│    └─ReLU: 2-18                        [1, 12, 7, 7]             --\n",
        "│    └─BatchNorm2d: 2-19                 [1, 12, 7, 7]             24\n",
        "│    └─MaxPool2d: 2-20                   [1, 12, 3, 3]             --\n",
        "│    └─Dropout: 2-21                     [1, 12, 3, 3]             --\n",
        "├─Linear: 1-4                            [1, 10]                   1,090\n",
        "==========================================================================================\n",
        "Total params: 1,774\n",
        "Trainable params: 1,774\n",
        "Non-trainable params: 0\n",
        "Total mult-adds (Units.MEGABYTES): 0.15\n",
        "==========================================================================================\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 0.16\n",
        "Params size (MB): 0.01\n",
        "Estimated Total Size (MB): 0.17\n",
        "==========================================================================================\n",
        "\n",
        "git commit ID 2c693e8 for training loop\n",
        "Final Results:\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.3, initial_div=20, final_div=50, warmup_pct=0.3, momentum=0.9, weight_decay=1e-05\n",
        "Best Accuracy: 99.02%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.2, initial_div=20, final_div=50, warmup_pct=0.4, momentum=0.95, weight_decay=1e-05\n",
        "Best Accuracy: 99.02%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.4, initial_div=25, final_div=175, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 98.95%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.45, initial_div=30, final_div=200, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 99.03%\n",
        "--------------------------------------------------------------------------------\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Depthwise Separable Convolutions Model\n",
        "class DepthwiseSeparableConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
        "        super(DepthwiseSeparableConv, self).__init__()\n",
        "        # Depthwise convolution\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, \n",
        "                                   padding=padding, groups=in_channels, bias=False)\n",
        "        # Pointwise convolution\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.depthwise(x)\n",
        "        out = self.pointwise(out)\n",
        "        return out\n",
        "# Depthwise Separable Convolutions Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "         \n",
        "        self.conv1 = nn.Sequential(\n",
        "            DepthwiseSeparableConv(1, 4, kernel_size=3, padding=1),  # Reduced parameters\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(4),\n",
        "            DepthwiseSeparableConv(4, 4, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            DepthwiseSeparableConv(4, 8, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            DepthwiseSeparableConv(8, 8, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            DepthwiseSeparableConv(8, 12, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(12 * 3 * 3, 10)  # Output layer remains unchanged\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 12 * 3 * 3)  # Flatten\n",
        "        x = F.dropout(x, p=0.0)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "Net                                      [1, 10]                   --\n",
        "├─Sequential: 1-1                        [1, 4, 14, 14]            --\n",
        "│    └─DepthwiseSeparableConv: 2-1       [1, 4, 28, 28]            --\n",
        "│    │    └─Conv2d: 3-1                  [1, 1, 28, 28]            9\n",
        "│    │    └─Conv2d: 3-2                  [1, 4, 28, 28]            4\n",
        "│    └─ReLU: 2-2                         [1, 4, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-3                  [1, 4, 28, 28]            8\n",
        "│    └─DepthwiseSeparableConv: 2-4       [1, 4, 28, 28]            --\n",
        "│    │    └─Conv2d: 3-3                  [1, 4, 28, 28]            36\n",
        "│    │    └─Conv2d: 3-4                  [1, 4, 28, 28]            16\n",
        "│    └─ReLU: 2-5                         [1, 4, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-6                  [1, 4, 28, 28]            8\n",
        "│    └─MaxPool2d: 2-7                    [1, 4, 14, 14]            --\n",
        "│    └─Dropout: 2-8                      [1, 4, 14, 14]            --\n",
        "├─Sequential: 1-2                        [1, 8, 7, 7]              --\n",
        "│    └─DepthwiseSeparableConv: 2-9       [1, 8, 14, 14]            --\n",
        "│    │    └─Conv2d: 3-5                  [1, 4, 14, 14]            36\n",
        "│    │    └─Conv2d: 3-6                  [1, 8, 14, 14]            32\n",
        "│    └─ReLU: 2-10                        [1, 8, 14, 14]            --\n",
        "│    └─BatchNorm2d: 2-11                 [1, 8, 14, 14]            16\n",
        "│    └─DepthwiseSeparableConv: 2-12      [1, 8, 14, 14]            --\n",
        "│    │    └─Conv2d: 3-7                  [1, 8, 14, 14]            72\n",
        "│    │    └─Conv2d: 3-8                  [1, 8, 14, 14]            64\n",
        "│    └─ReLU: 2-13                        [1, 8, 14, 14]            --\n",
        "│    └─BatchNorm2d: 2-14                 [1, 8, 14, 14]            16\n",
        "│    └─MaxPool2d: 2-15                   [1, 8, 7, 7]              --\n",
        "│    └─Dropout: 2-16                     [1, 8, 7, 7]              --\n",
        "├─Sequential: 1-3                        [1, 12, 3, 3]             --\n",
        "│    └─DepthwiseSeparableConv: 2-17      [1, 12, 7, 7]             --\n",
        "│    │    └─Conv2d: 3-9                  [1, 8, 7, 7]              72\n",
        "│    │    └─Conv2d: 3-10                 [1, 12, 7, 7]             96\n",
        "│    └─ReLU: 2-18                        [1, 12, 7, 7]             --\n",
        "│    └─BatchNorm2d: 2-19                 [1, 12, 7, 7]             24\n",
        "│    └─MaxPool2d: 2-20                   [1, 12, 3, 3]             --\n",
        "│    └─Dropout: 2-21                     [1, 12, 3, 3]             --\n",
        "├─Linear: 1-4                            [1, 10]                   1,090\n",
        "==========================================================================================\n",
        "Total params: 1,599\n",
        "Trainable params: 1,599\n",
        "Non-trainable params: 0\n",
        "Total mult-adds (Units.MEGABYTES): 0.10\n",
        "==========================================================================================\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 0.21\n",
        "Params size (MB): 0.01\n",
        "Estimated Total Size (MB): 0.22\n",
        "==========================================================================================\n",
        "\n",
        "Final Results:\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.3, initial_div=20, final_div=50, warmup_pct=0.3, momentum=0.9, weight_decay=1e-05\n",
        "Best Accuracy: 98.60%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.2, initial_div=20, final_div=50, warmup_pct=0.4, momentum=0.95, weight_decay=1e-05\n",
        "Best Accuracy: 98.52%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.4, initial_div=25, final_div=175, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 98.78%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.45, initial_div=30, final_div=200, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 98.58%\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Residual Block Definition\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            # Reduced to a single conv layer to save parameters\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(channels)\n",
        "        )\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# ResNet-Inspired Model with reduced parameters\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # Reduced initial channels from 4 to 3\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 3, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(3),\n",
        "            ResidualBlock(3),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        # Reduced middle channels from 8 to 6\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(3, 6, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(6),\n",
        "            ResidualBlock(6),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        # Reduced final channels from 12 to 8\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(6, 8, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            ResidualBlock(8),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        # Final linear layer now takes 8 * 3 * 3 input features\n",
        "        self.fc1 = nn.Linear(8 * 3 * 3, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 8 * 3 * 3)\n",
        "        x = F.dropout(x, p=0.0)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "        ==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "Net                                      [1, 10]                   --\n",
        "├─Sequential: 1-1                        [1, 3, 14, 14]            --\n",
        "│    └─Conv2d: 2-1                       [1, 3, 28, 28]            27\n",
        "│    └─ReLU: 2-2                         [1, 3, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-3                  [1, 3, 28, 28]            6\n",
        "│    └─ResidualBlock: 2-4                [1, 3, 28, 28]            --\n",
        "│    │    └─Sequential: 3-1              [1, 3, 28, 28]            87\n",
        "│    │    └─ReLU: 3-2                    [1, 3, 28, 28]            --\n",
        "│    └─MaxPool2d: 2-5                    [1, 3, 14, 14]            --\n",
        "│    └─Dropout: 2-6                      [1, 3, 14, 14]            --\n",
        "├─Sequential: 1-2                        [1, 6, 7, 7]              --\n",
        "│    └─Conv2d: 2-7                       [1, 6, 14, 14]            162\n",
        "│    └─ReLU: 2-8                         [1, 6, 14, 14]            --\n",
        "│    └─BatchNorm2d: 2-9                  [1, 6, 14, 14]            12\n",
        "│    └─ResidualBlock: 2-10               [1, 6, 14, 14]            --\n",
        "│    │    └─Sequential: 3-3              [1, 6, 14, 14]            336\n",
        "│    │    └─ReLU: 3-4                    [1, 6, 14, 14]            --\n",
        "│    └─MaxPool2d: 2-11                   [1, 6, 7, 7]              --\n",
        "│    └─Dropout: 2-12                     [1, 6, 7, 7]              --\n",
        "├─Sequential: 1-3                        [1, 8, 3, 3]              --\n",
        "│    └─Conv2d: 2-13                      [1, 8, 7, 7]              432\n",
        "│    └─ReLU: 2-14                        [1, 8, 7, 7]              --\n",
        "│    └─BatchNorm2d: 2-15                 [1, 8, 7, 7]              16\n",
        "│    └─ResidualBlock: 2-16               [1, 8, 7, 7]              --\n",
        "│    │    └─Sequential: 3-5              [1, 8, 7, 7]              592\n",
        "│    │    └─ReLU: 3-6                    [1, 8, 7, 7]              --\n",
        "│    └─MaxPool2d: 2-17                   [1, 8, 3, 3]              --\n",
        "│    └─Dropout: 2-18                     [1, 8, 3, 3]              --\n",
        "├─Linear: 1-4                            [1, 10]                   730\n",
        "==========================================================================================\n",
        "Total params: 2,400\n",
        "Trainable params: 2,400\n",
        "Non-trainable params: 0\n",
        "Total mult-adds (Units.MEGABYTES): 0.23\n",
        "==========================================================================================\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 0.13\n",
        "Params size (MB): 0.01\n",
        "Estimated Total Size (MB): 0.14\n",
        "\n",
        "        Final Results:\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.3, initial_div=20, final_div=50, warmup_pct=0.3, momentum=0.9, weight_decay=1e-05\n",
        "Best Accuracy: 99.14%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.2, initial_div=20, final_div=50, warmup_pct=0.4, momentum=0.95, weight_decay=1e-05\n",
        "Best Accuracy: 99.19%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.4, initial_div=25, final_div=175, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 99.12%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.45, initial_div=30, final_div=200, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 99.04%\n",
        "--------------------------------------------------------------------------------\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "4. # Squeeze-and-Excitation Block Definition\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, max(channels // reduction, 1), bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(max(channels // reduction, 1), channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "# SE-Enhanced Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(4),\n",
        "            SEBlock(4),\n",
        "            nn.Conv2d(4, 4, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(4),\n",
        "            SEBlock(4),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(4, 8, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            SEBlock(8),\n",
        "            nn.Conv2d(8, 8, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            SEBlock(8),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 12, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(12),\n",
        "            SEBlock(12),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(12 * 3 * 3, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 12 * 3 * 3)\n",
        "        x = F.dropout(x, p=0.0)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "Net                                      [1, 10]                   --\n",
        "├─Sequential: 1-1                        [1, 4, 14, 14]            --\n",
        "│    └─Conv2d: 2-1                       [1, 4, 28, 28]            36\n",
        "│    └─ReLU: 2-2                         [1, 4, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-3                  [1, 4, 28, 28]            8\n",
        "│    └─SEBlock: 2-4                      [1, 4, 28, 28]            --\n",
        "│    │    └─AdaptiveAvgPool2d: 3-1       [1, 4, 1, 1]              --\n",
        "│    │    └─Sequential: 3-2              [1, 4]                    8\n",
        "│    └─Conv2d: 2-5                       [1, 4, 28, 28]            144\n",
        "│    └─ReLU: 2-6                         [1, 4, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-7                  [1, 4, 28, 28]            8\n",
        "│    └─SEBlock: 2-8                      [1, 4, 28, 28]            --\n",
        "│    │    └─AdaptiveAvgPool2d: 3-3       [1, 4, 1, 1]              --\n",
        "│    │    └─Sequential: 3-4              [1, 4]                    8\n",
        "│    └─MaxPool2d: 2-9                    [1, 4, 14, 14]            --\n",
        "│    └─Dropout: 2-10                     [1, 4, 14, 14]            --\n",
        "├─Sequential: 1-2                        [1, 8, 7, 7]              --\n",
        "│    └─Conv2d: 2-11                      [1, 8, 14, 14]            288\n",
        "│    └─ReLU: 2-12                        [1, 8, 14, 14]            --\n",
        "│    └─BatchNorm2d: 2-13                 [1, 8, 14, 14]            16\n",
        "│    └─SEBlock: 2-14                     [1, 8, 14, 14]            --\n",
        "│    │    └─AdaptiveAvgPool2d: 3-5       [1, 8, 1, 1]              --\n",
        "│    │    └─Sequential: 3-6              [1, 8]                    16\n",
        "│    └─Conv2d: 2-15                      [1, 8, 14, 14]            576\n",
        "│    └─ReLU: 2-16                        [1, 8, 14, 14]            --\n",
        "│    └─BatchNorm2d: 2-17                 [1, 8, 14, 14]            16\n",
        "│    └─SEBlock: 2-18                     [1, 8, 14, 14]            --\n",
        "│    │    └─AdaptiveAvgPool2d: 3-7       [1, 8, 1, 1]              --\n",
        "│    │    └─Sequential: 3-8              [1, 8]                    16\n",
        "│    └─MaxPool2d: 2-19                   [1, 8, 7, 7]              --\n",
        "│    └─Dropout: 2-20                     [1, 8, 7, 7]              --\n",
        "├─Sequential: 1-3                        [1, 12, 3, 3]             --\n",
        "│    └─Conv2d: 2-21                      [1, 12, 7, 7]             864\n",
        "│    └─ReLU: 2-22                        [1, 12, 7, 7]             --\n",
        "│    └─BatchNorm2d: 2-23                 [1, 12, 7, 7]             24\n",
        "│    └─SEBlock: 2-24                     [1, 12, 7, 7]             --\n",
        "│    │    └─AdaptiveAvgPool2d: 3-9       [1, 12, 1, 1]             --\n",
        "│    │    └─Sequential: 3-10             [1, 12]                   24\n",
        "│    └─MaxPool2d: 2-25                   [1, 12, 3, 3]             --\n",
        "│    └─Dropout: 2-26                     [1, 12, 3, 3]             --\n",
        "├─Linear: 1-4                            [1, 10]                   1,090\n",
        "==========================================================================================\n",
        "Total params: 3,142\n",
        "Trainable params: 3,142\n",
        "Non-trainable params: 0\n",
        "Total mult-adds (Units.MEGABYTES): 0.35\n",
        "==========================================================================================\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 0.16\n",
        "Params size (MB): 0.01\n",
        "Estimated Total Size (MB): 0.18\n",
        "==========================================================================================\n",
        "\n",
        "\n",
        "Final Results:\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.3, initial_div=20, final_div=50, warmup_pct=0.3, momentum=0.9, weight_decay=1e-05\n",
        "Best Accuracy: 99.28%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.2, initial_div=20, final_div=50, warmup_pct=0.4, momentum=0.95, weight_decay=1e-05\n",
        "Best Accuracy: 99.20%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.4, initial_div=25, final_div=175, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 99.37%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.45, initial_div=30, final_div=200, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 99.31%\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# 5. Inverted Residual Block Definition\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, expansion=6):\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        hidden_dim = in_channels * expansion\n",
        "        self.use_res_connect = in_channels == out_channels  # Only use residual if input/output channels match\n",
        "        \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False),  # Expansion\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1, groups=hidden_dim, bias=False),  # Depthwise\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, bias=False)  # Projection\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return self.conv(x) + x  # Only add residual if channels match\n",
        "        return self.conv(x)  # Otherwise just return the convolution output\n",
        "\n",
        "# Inverted Residuals Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            InvertedResidual(4, 4, expansion=1),  # No expansion to keep parameters low\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            InvertedResidual(4, 8, expansion=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(8),\n",
        "            InvertedResidual(8, 8, expansion=1),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            InvertedResidual(8, 12, expansion=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(12 * 3 * 3, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 12 * 3 * 3)\n",
        "        x = F.dropout(x, p=0.0)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "Net                                      [1, 10]                   --\n",
        "├─Sequential: 1-1                        [1, 4, 14, 14]            --\n",
        "│    └─Conv2d: 2-1                       [1, 4, 28, 28]            36\n",
        "│    └─ReLU: 2-2                         [1, 4, 28, 28]            --\n",
        "│    └─InvertedResidual: 2-3             [1, 4, 28, 28]            --\n",
        "│    │    └─Sequential: 3-1              [1, 4, 28, 28]            68\n",
        "│    └─MaxPool2d: 2-4                    [1, 4, 14, 14]            --\n",
        "│    └─Dropout: 2-5                      [1, 4, 14, 14]            --\n",
        "├─Sequential: 1-2                        [1, 8, 7, 7]              --\n",
        "│    └─InvertedResidual: 2-6             [1, 8, 14, 14]            --\n",
        "│    │    └─Sequential: 3-2              [1, 8, 14, 14]            168\n",
        "│    └─ReLU: 2-7                         [1, 8, 14, 14]            --\n",
        "│    └─BatchNorm2d: 2-8                  [1, 8, 14, 14]            16\n",
        "│    └─InvertedResidual: 2-9             [1, 8, 14, 14]            --\n",
        "│    │    └─Sequential: 3-3              [1, 8, 14, 14]            200\n",
        "│    └─MaxPool2d: 2-10                   [1, 8, 7, 7]              --\n",
        "│    └─Dropout: 2-11                     [1, 8, 7, 7]              --\n",
        "├─Sequential: 1-3                        [1, 12, 3, 3]             --\n",
        "│    └─InvertedResidual: 2-12            [1, 12, 7, 7]             --\n",
        "│    │    └─Sequential: 3-4              [1, 12, 7, 7]             464\n",
        "│    └─ReLU: 2-13                        [1, 12, 7, 7]             --\n",
        "│    └─BatchNorm2d: 2-14                 [1, 12, 7, 7]             24\n",
        "│    └─MaxPool2d: 2-15                   [1, 12, 3, 3]             --\n",
        "│    └─Dropout: 2-16                     [1, 12, 3, 3]             --\n",
        "├─Linear: 1-4                            [1, 10]                   1,090\n",
        "==========================================================================================\n",
        "Total params: 2,066\n",
        "Trainable params: 2,066\n",
        "Non-trainable params: 0\n",
        "Total mult-adds (Units.MEGABYTES): 0.18\n",
        "==========================================================================================\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 0.21\n",
        "Params size (MB): 0.01\n",
        "Estimated Total Size (MB): 0.22\n",
        "\n",
        "\n",
        "Final Results:\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.3, initial_div=20, final_div=50, warmup_pct=0.3, momentum=0.9, weight_decay=1e-05\n",
        "Best Accuracy: 99.11%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.2, initial_div=20, final_div=50, warmup_pct=0.4, momentum=0.95, weight_decay=1e-05\n",
        "Best Accuracy: 99.03%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.4, initial_div=25, final_div=175, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 98.76%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.45, initial_div=30, final_div=200, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 99.00%\n",
        "--------------------------------------------------------------------------------   \n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# 6 Leaky ReLU Activation Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, kernel_size=3, padding=1, bias=False),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.Conv2d(4, 4, kernel_size=3, padding=1, bias=False),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(4, 8, kernel_size=3, padding=1, bias=False),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.Conv2d(8, 8, kernel_size=3, padding=1, bias=False),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 12, kernel_size=3, padding=1, bias=False),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(12 * 3 * 3, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 12 * 3 * 3)\n",
        "        x = F.dropout(x, p=0.0)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "Net                                      [1, 10]                   --\n",
        "├─Sequential: 1-1                        [1, 4, 14, 14]            --\n",
        "│    └─Conv2d: 2-1                       [1, 4, 28, 28]            36\n",
        "│    └─LeakyReLU: 2-2                    [1, 4, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-3                  [1, 4, 28, 28]            8\n",
        "│    └─Conv2d: 2-4                       [1, 4, 28, 28]            144\n",
        "│    └─LeakyReLU: 2-5                    [1, 4, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-6                  [1, 4, 28, 28]            8\n",
        "│    └─MaxPool2d: 2-7                    [1, 4, 14, 14]            --\n",
        "│    └─Dropout: 2-8                      [1, 4, 14, 14]            --\n",
        "├─Sequential: 1-2                        [1, 8, 7, 7]              --\n",
        "│    └─Conv2d: 2-9                       [1, 8, 14, 14]            288\n",
        "│    └─LeakyReLU: 2-10                   [1, 8, 14, 14]            --\n",
        "│    └─BatchNorm2d: 2-11                 [1, 8, 14, 14]            16\n",
        "│    └─Conv2d: 2-12                      [1, 8, 14, 14]            576\n",
        "│    └─LeakyReLU: 2-13                   [1, 8, 14, 14]            --\n",
        "│    └─BatchNorm2d: 2-14                 [1, 8, 14, 14]            16\n",
        "│    └─MaxPool2d: 2-15                   [1, 8, 7, 7]              --\n",
        "│    └─Dropout: 2-16                     [1, 8, 7, 7]              --\n",
        "├─Sequential: 1-3                        [1, 12, 3, 3]             --\n",
        "│    └─Conv2d: 2-17                      [1, 12, 7, 7]             864\n",
        "│    └─LeakyReLU: 2-18                   [1, 12, 7, 7]             --\n",
        "│    └─BatchNorm2d: 2-19                 [1, 12, 7, 7]             24\n",
        "│    └─MaxPool2d: 2-20                   [1, 12, 3, 3]             --\n",
        "│    └─Dropout: 2-21                     [1, 12, 3, 3]             --\n",
        "├─Linear: 1-4                            [1, 10]                   1,090\n",
        "==========================================================================================\n",
        "Total params: 3,070\n",
        "Trainable params: 3,070\n",
        "Non-trainable params: 0\n",
        "Total mult-adds (Units.MEGABYTES): 0.35\n",
        "==========================================================================================\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 0.16\n",
        "Params size (MB): 0.01\n",
        "Estimated Total Size (MB): 0.18\n",
        "==========================================================================================\n",
        "Final Results:\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.3, initial_div=20, final_div=50, warmup_pct=0.3, momentum=0.9, weight_decay=1e-05\n",
        "Best Accuracy: 99.24%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.2, initial_div=20, final_div=50, warmup_pct=0.4, momentum=0.95, weight_decay=1e-05\n",
        "Best Accuracy: 99.31%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.4, initial_div=25, final_div=175, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 99.26%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.45, initial_div=30, final_div=200, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 99.35%\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# 7 Bottleneck-Incorporated Model\n",
        "# Bottleneck Layer Definition\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, bottleneck_channels, out_channels):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, bottleneck_channels, kernel_size=1, bias=False),  # Reduce channels\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(bottleneck_channels, bottleneck_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(bottleneck_channels, out_channels, kernel_size=1, bias=False)  # Expand back\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "# Bottleneck-Incorporated Model\n",
        "class Net_Bottleneck(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net_Bottleneck, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            Bottleneck(1, 2, 4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(4),\n",
        "            Bottleneck(4, 2, 4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            Bottleneck(4, 2, 8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(8),\n",
        "            Bottleneck(8, 2, 8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            Bottleneck(8, 2, 12),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(12 * 3 * 3, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 12 * 3 * 3)\n",
        "        x = F.dropout(x, p=0.0)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "Net                                      [1, 10]                   --\n",
        "├─Sequential: 1-1                        [1, 4, 14, 14]            --\n",
        "│    └─Bottleneck: 2-1                   [1, 4, 28, 28]            --\n",
        "│    │    └─Sequential: 3-1              [1, 4, 28, 28]            46\n",
        "│    └─ReLU: 2-2                         [1, 4, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-3                  [1, 4, 28, 28]            8\n",
        "│    └─Bottleneck: 2-4                   [1, 4, 28, 28]            --\n",
        "│    │    └─Sequential: 3-2              [1, 4, 28, 28]            52\n",
        "│    └─ReLU: 2-5                         [1, 4, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-6                  [1, 4, 28, 28]            8\n",
        "│    └─MaxPool2d: 2-7                    [1, 4, 14, 14]            --\n",
        "│    └─Dropout: 2-8                      [1, 4, 14, 14]            --\n",
        "├─Sequential: 1-2                        [1, 8, 7, 7]              --\n",
        "│    └─Bottleneck: 2-9                   [1, 8, 14, 14]            --\n",
        "│    │    └─Sequential: 3-3              [1, 8, 14, 14]            60\n",
        "│    └─ReLU: 2-10                        [1, 8, 14, 14]            --\n",
        "│    └─BatchNorm2d: 2-11                 [1, 8, 14, 14]            16\n",
        "│    └─Bottleneck: 2-12                  [1, 8, 14, 14]            --\n",
        "│    │    └─Sequential: 3-4              [1, 8, 14, 14]            68\n",
        "│    └─ReLU: 2-13                        [1, 8, 14, 14]            --\n",
        "│    └─BatchNorm2d: 2-14                 [1, 8, 14, 14]            16\n",
        "│    └─MaxPool2d: 2-15                   [1, 8, 7, 7]              --\n",
        "│    └─Dropout: 2-16                     [1, 8, 7, 7]              --\n",
        "├─Sequential: 1-3                        [1, 12, 3, 3]             --\n",
        "│    └─Bottleneck: 2-17                  [1, 12, 7, 7]             --\n",
        "│    │    └─Sequential: 3-5              [1, 12, 7, 7]             76\n",
        "│    └─ReLU: 2-18                        [1, 12, 7, 7]             --\n",
        "│    └─BatchNorm2d: 2-19                 [1, 12, 7, 7]             24\n",
        "│    └─MaxPool2d: 2-20                   [1, 12, 3, 3]             --\n",
        "│    └─Dropout: 2-21                     [1, 12, 3, 3]             --\n",
        "├─Linear: 1-4                            [1, 10]                   1,090\n",
        "==========================================================================================\n",
        "Total params: 1,464\n",
        "Trainable params: 1,464\n",
        "Non-trainable params: 0\n",
        "Total mult-adds (Units.MEGABYTES): 0.11\n",
        "==========================================================================================\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 0.22\n",
        "Params size (MB): 0.01\n",
        "Estimated Total Size (MB): 0.23\n",
        "==========================================================================================\n",
        "\n",
        "Final Results:\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.3, initial_div=20, final_div=50, warmup_pct=0.3, momentum=0.9, weight_decay=1e-05\n",
        "Best Accuracy: 94.54%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.2, initial_div=20, final_div=50, warmup_pct=0.4, momentum=0.95, weight_decay=1e-05\n",
        "Best Accuracy: 11.35%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.4, initial_div=25, final_div=175, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 97.08%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.45, initial_div=30, final_div=200, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 95.48%\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# 8 # Make sure to install dropblock:\n",
        "# pip install dropblock\n",
        "\n",
        "from dropblock import DropBlock2D\n",
        "\n",
        "# DropBlock-Regularized Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(4),\n",
        "            DropBlock2D(block_size=7, drop_prob=0.1),  # DropBlock after first conv\n",
        "            nn.Conv2d(4, 4, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(4),\n",
        "            DropBlock2D(block_size=5, drop_prob=0.05),  # DropBlock after second conv\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(4, 8, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(8),\n",
        "            DropBlock2D(block_size=7, drop_prob=0.1),\n",
        "            nn.Conv2d(8, 8, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(8),\n",
        "            DropBlock2D(block_size=5, drop_prob=0.05),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 12, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(12),\n",
        "            DropBlock2D(block_size=7, drop_prob=0.1),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(12 * 3 * 3, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 12 * 3 * 3)\n",
        "        x = F.dropout(x, p=0.0)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "Net                                      [1, 10]                   --\n",
        "├─Sequential: 1-1                        [1, 4, 14, 14]            --\n",
        "│    └─Conv2d: 2-1                       [1, 4, 28, 28]            36\n",
        "│    └─ReLU: 2-2                         [1, 4, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-3                  [1, 4, 28, 28]            8\n",
        "│    └─DropBlock2D: 2-4                  [1, 4, 28, 28]            --\n",
        "│    └─Conv2d: 2-5                       [1, 4, 28, 28]            144\n",
        "│    └─ReLU: 2-6                         [1, 4, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-7                  [1, 4, 28, 28]            8\n",
        "│    └─DropBlock2D: 2-8                  [1, 4, 28, 28]            --\n",
        "│    └─MaxPool2d: 2-9                    [1, 4, 14, 14]            --\n",
        "│    └─Dropout: 2-10                     [1, 4, 14, 14]            --\n",
        "├─Sequential: 1-2                        [1, 8, 7, 7]              --\n",
        "│    └─Conv2d: 2-11                      [1, 8, 14, 14]            288\n",
        "│    └─ReLU: 2-12                        [1, 8, 14, 14]            --\n",
        "│    └─BatchNorm2d: 2-13                 [1, 8, 14, 14]            16\n",
        "│    └─DropBlock2D: 2-14                 [1, 8, 14, 14]            --\n",
        "│    └─Conv2d: 2-15                      [1, 8, 14, 14]            576\n",
        "│    └─ReLU: 2-16                        [1, 8, 14, 14]            --\n",
        "│    └─BatchNorm2d: 2-17                 [1, 8, 14, 14]            16\n",
        "│    └─DropBlock2D: 2-18                 [1, 8, 14, 14]            --\n",
        "│    └─MaxPool2d: 2-19                   [1, 8, 7, 7]              --\n",
        "│    └─Dropout: 2-20                     [1, 8, 7, 7]              --\n",
        "├─Sequential: 1-3                        [1, 12, 3, 3]             --\n",
        "│    └─Conv2d: 2-21                      [1, 12, 7, 7]             864\n",
        "│    └─ReLU: 2-22                        [1, 12, 7, 7]             --\n",
        "│    └─BatchNorm2d: 2-23                 [1, 12, 7, 7]             24\n",
        "│    └─DropBlock2D: 2-24                 [1, 12, 7, 7]             --\n",
        "│    └─MaxPool2d: 2-25                   [1, 12, 3, 3]             --\n",
        "│    └─Dropout: 2-26                     [1, 12, 3, 3]             --\n",
        "├─Linear: 1-4                            [1, 10]                   1,090\n",
        "==========================================================================================\n",
        "Total params: 3,070\n",
        "Trainable params: 3,070\n",
        "Non-trainable params: 0\n",
        "Total mult-adds (Units.MEGABYTES): 0.35\n",
        "==========================================================================================\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 0.16\n",
        "Params size (MB): 0.01\n",
        "Estimated Total Size (MB): 0.18\n",
        "==========================================================================================\n",
        "\n",
        "Final Results:\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.3, initial_div=20, final_div=50, warmup_pct=0.3, momentum=0.9, weight_decay=1e-05\n",
        "Best Accuracy: 98.84%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.2, initial_div=20, final_div=50, warmup_pct=0.4, momentum=0.95, weight_decay=1e-05\n",
        "Best Accuracy: 99.08%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.4, initial_div=25, final_div=175, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 99.02%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.45, initial_div=30, final_div=200, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 99.06%\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# 9 # Attention Block Definition\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Conv2d(channels, max(1, channels // 8), kernel_size=1, bias=False),  # Ensure at least 1 output channel\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(max(1, channels // 8), channels, kernel_size=1, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Keep the spatial dimensions when applying adaptive_avg_pool2d\n",
        "        y = F.adaptive_avg_pool2d(x, 1)  # This will output (batch_size, channels, 1, 1)\n",
        "        y = self.attention(y)  # Apply attention directly to the 4D tensor\n",
        "        return x * y  # Broadcasting will work correctly with 4D tensors\n",
        "\n",
        "# Attention-Enhanced Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(4),\n",
        "            AttentionBlock(4),\n",
        "            nn.Conv2d(4, 4, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(4),\n",
        "            AttentionBlock(4),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(4, 8, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(8),\n",
        "            AttentionBlock(8),\n",
        "            nn.Conv2d(8, 8, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(8),\n",
        "            AttentionBlock(8),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 12, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(12),\n",
        "            AttentionBlock(12),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(12 * 3 * 3, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 12 * 3 * 3)\n",
        "        x = F.dropout(x, p=0.0)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "Net                                      [1, 10]                   --\n",
        "├─Sequential: 1-1                        [1, 4, 14, 14]            --\n",
        "│    └─Conv2d: 2-1                       [1, 4, 28, 28]            36\n",
        "│    └─ReLU: 2-2                         [1, 4, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-3                  [1, 4, 28, 28]            8\n",
        "│    └─AttentionBlock: 2-4               [1, 4, 28, 28]            --\n",
        "│    │    └─Sequential: 3-1              [1, 4, 1, 1]              8\n",
        "│    └─Conv2d: 2-5                       [1, 4, 28, 28]            144\n",
        "│    └─ReLU: 2-6                         [1, 4, 28, 28]            --\n",
        "│    └─BatchNorm2d: 2-7                  [1, 4, 28, 28]            8\n",
        "│    └─AttentionBlock: 2-8               [1, 4, 28, 28]            --\n",
        "│    │    └─Sequential: 3-2              [1, 4, 1, 1]              8\n",
        "│    └─MaxPool2d: 2-9                    [1, 4, 14, 14]            --\n",
        "│    └─Dropout: 2-10                     [1, 4, 14, 14]            --\n",
        "├─Sequential: 1-2                        [1, 8, 7, 7]              --\n",
        "│    └─Conv2d: 2-11                      [1, 8, 14, 14]            288\n",
        "│    └─ReLU: 2-12                        [1, 8, 14, 14]            --\n",
        "│    └─BatchNorm2d: 2-13                 [1, 8, 14, 14]            16\n",
        "│    └─AttentionBlock: 2-14              [1, 8, 14, 14]            --\n",
        "│    │    └─Sequential: 3-3              [1, 8, 1, 1]              16\n",
        "│    └─Conv2d: 2-15                      [1, 8, 14, 14]            576\n",
        "│    └─ReLU: 2-16                        [1, 8, 14, 14]            --\n",
        "│    └─BatchNorm2d: 2-17                 [1, 8, 14, 14]            16\n",
        "│    └─AttentionBlock: 2-18              [1, 8, 14, 14]            --\n",
        "│    │    └─Sequential: 3-4              [1, 8, 1, 1]              16\n",
        "│    └─MaxPool2d: 2-19                   [1, 8, 7, 7]              --\n",
        "│    └─Dropout: 2-20                     [1, 8, 7, 7]              --\n",
        "├─Sequential: 1-3                        [1, 12, 3, 3]             --\n",
        "│    └─Conv2d: 2-21                      [1, 12, 7, 7]             864\n",
        "│    └─ReLU: 2-22                        [1, 12, 7, 7]             --\n",
        "│    └─BatchNorm2d: 2-23                 [1, 12, 7, 7]             24\n",
        "│    └─AttentionBlock: 2-24              [1, 12, 7, 7]             --\n",
        "│    │    └─Sequential: 3-5              [1, 12, 1, 1]             24\n",
        "│    └─MaxPool2d: 2-25                   [1, 12, 3, 3]             --\n",
        "│    └─Dropout: 2-26                     [1, 12, 3, 3]             --\n",
        "├─Linear: 1-4                            [1, 10]                   1,090\n",
        "==========================================================================================\n",
        "Total params: 3,142\n",
        "Trainable params: 3,142\n",
        "Non-trainable params: 0\n",
        "Total mult-adds (Units.MEGABYTES): 0.35\n",
        "==========================================================================================\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 0.16\n",
        "Params size (MB): 0.01\n",
        "Estimated Total Size (MB): 0.18\n",
        "==========================================================================================\n",
        "\n",
        "Final Results:\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.3, initial_div=20, final_div=50, warmup_pct=0.3, momentum=0.9, weight_decay=1e-05\n",
        "Best Accuracy: 99.18%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.2, initial_div=20, final_div=50, warmup_pct=0.4, momentum=0.95, weight_decay=1e-05\n",
        "Best Accuracy: 99.24%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.4, initial_div=25, final_div=175, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 99.32%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.45, initial_div=30, final_div=200, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 99.32%\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# 10 # Mixed Architectural Innovations Model\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Conv2d(channels, max(1, channels // 8), kernel_size=1, bias=False),  # Ensure at least 1 output channel\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(max(1, channels // 8), channels, kernel_size=1, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Keep the spatial dimensions when applying adaptive_avg_pool2d\n",
        "        y = F.adaptive_avg_pool2d(x, 1)  # This will output (batch_size, channels, 1, 1)\n",
        "        y = self.attention(y)  # Apply attention directly to the 4D tensor\n",
        "        return x * y  # Broadcasting will work correctly with 4D tensors\n",
        "\n",
        "# Residual Block Definition\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            # Reduced to a single conv layer to save parameters\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(channels)\n",
        "        )\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "# Depthwise Separable Convolutions Model\n",
        "class DepthwiseSeparableConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
        "        super(DepthwiseSeparableConv, self).__init__()\n",
        "        # Depthwise convolution\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, \n",
        "                                   padding=padding, groups=in_channels, bias=False)\n",
        "        # Pointwise convolution\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.depthwise(x)\n",
        "        out = self.pointwise(out)\n",
        "        return out\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, max(channels // reduction, 1), bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(max(channels // reduction, 1), channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # First Convolutional Block with SE and Residual\n",
        "        self.conv1 = nn.Sequential(\n",
        "            DepthwiseSeparableConv(1, 4, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            SEBlock(4),\n",
        "            ResidualBlock(4),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        # Second Convolutional Block with Attention and Residual\n",
        "        self.conv2 = nn.Sequential(\n",
        "            DepthwiseSeparableConv(4, 8, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            AttentionBlock(8),\n",
        "            ResidualBlock(8),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        # Third Convolutional Block with SE and Depthwise Separable Conv\n",
        "        self.conv3 = nn.Sequential(\n",
        "            DepthwiseSeparableConv(8, 12, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            SEBlock(12),\n",
        "            ResidualBlock(12),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.05)\n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Linear(12 * 3 * 3, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 12 * 3 * 3)\n",
        "        x = F.dropout(x, p=0.0)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "    \n",
        "        \n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "Net                                      [1, 10]                   --\n",
        "├─Sequential: 1-1                        [1, 4, 14, 14]            --\n",
        "│    └─DepthwiseSeparableConv: 2-1       [1, 4, 28, 28]            --\n",
        "│    │    └─Conv2d: 3-1                  [1, 1, 28, 28]            9\n",
        "│    │    └─Conv2d: 3-2                  [1, 4, 28, 28]            4\n",
        "│    └─ReLU: 2-2                         [1, 4, 28, 28]            --\n",
        "│    └─SEBlock: 2-3                      [1, 4, 28, 28]            --\n",
        "│    │    └─AdaptiveAvgPool2d: 3-3       [1, 4, 1, 1]              --\n",
        "│    │    └─Sequential: 3-4              [1, 4]                    8\n",
        "│    └─ResidualBlock: 2-4                [1, 4, 28, 28]            --\n",
        "│    │    └─Sequential: 3-5              [1, 4, 28, 28]            152\n",
        "│    │    └─ReLU: 3-6                    [1, 4, 28, 28]            --\n",
        "│    └─MaxPool2d: 2-5                    [1, 4, 14, 14]            --\n",
        "│    └─Dropout: 2-6                      [1, 4, 14, 14]            --\n",
        "├─Sequential: 1-2                        [1, 8, 7, 7]              --\n",
        "│    └─DepthwiseSeparableConv: 2-7       [1, 8, 14, 14]            --\n",
        "│    │    └─Conv2d: 3-7                  [1, 4, 14, 14]            36\n",
        "│    │    └─Conv2d: 3-8                  [1, 8, 14, 14]            32\n",
        "│    └─ReLU: 2-8                         [1, 8, 14, 14]            --\n",
        "│    └─AttentionBlock: 2-9               [1, 8, 14, 14]            --\n",
        "│    │    └─Sequential: 3-9              [1, 8, 1, 1]              16\n",
        "│    └─ResidualBlock: 2-10               [1, 8, 14, 14]            --\n",
        "│    │    └─Sequential: 3-10             [1, 8, 14, 14]            592\n",
        "│    │    └─ReLU: 3-11                   [1, 8, 14, 14]            --\n",
        "│    └─MaxPool2d: 2-11                   [1, 8, 7, 7]              --\n",
        "│    └─Dropout: 2-12                     [1, 8, 7, 7]              --\n",
        "├─Sequential: 1-3                        [1, 12, 3, 3]             --\n",
        "│    └─DepthwiseSeparableConv: 2-13      [1, 12, 7, 7]             --\n",
        "│    │    └─Conv2d: 3-12                 [1, 8, 7, 7]              72\n",
        "│    │    └─Conv2d: 3-13                 [1, 12, 7, 7]             96\n",
        "│    └─ReLU: 2-14                        [1, 12, 7, 7]             --\n",
        "│    └─SEBlock: 2-15                     [1, 12, 7, 7]             --\n",
        "│    │    └─AdaptiveAvgPool2d: 3-14      [1, 12, 1, 1]             --\n",
        "│    │    └─Sequential: 3-15             [1, 12]                   24\n",
        "│    └─ResidualBlock: 2-16               [1, 12, 7, 7]             --\n",
        "│    │    └─Sequential: 3-16             [1, 12, 7, 7]             1,320\n",
        "│    │    └─ReLU: 3-17                   [1, 12, 7, 7]             --\n",
        "│    └─MaxPool2d: 2-17                   [1, 12, 3, 3]             --\n",
        "│    └─Dropout: 2-18                     [1, 12, 3, 3]             --\n",
        "├─Linear: 1-4                            [1, 10]                   1,090\n",
        "==========================================================================================\n",
        "Total params: 3,451\n",
        "Trainable params: 3,451\n",
        "Non-trainable params: 0\n",
        "Total mult-adds (Units.MEGABYTES): 0.32\n",
        "==========================================================================================\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 0.14\n",
        "Params size (MB): 0.01\n",
        "Estimated Total Size (MB): 0.16\n",
        "==========================================================================================\n",
        "\n",
        "Final Results:\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.3, initial_div=20, final_div=50, warmup_pct=0.3, momentum=0.9, weight_decay=1e-05\n",
        "Best Accuracy: 99.13%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.2, initial_div=20, final_div=50, warmup_pct=0.4, momentum=0.95, weight_decay=1e-05\n",
        "Best Accuracy: 99.10%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.4, initial_div=25, final_div=175, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 99.08%\n",
        "--------------------------------------------------------------------------------\n",
        "Parameters: max_lr=0.45, initial_div=30, final_div=200, warmup_pct=0.5, momentum=0.9, weight_decay=0.0001\n",
        "Best Accuracy: 99.12%\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
