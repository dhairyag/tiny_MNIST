{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h_Cx9q2QFgM7"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, 3, padding=1),  # 1*3*3*4 + 4 = 40 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(4),              # 8 params\n",
        "            nn.Conv2d(4, 4, 3, padding=1),  # 4*3*3*4 + 4 = 148 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(4),              # 8 params\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(4, 8, 3, padding=1),  # 4*3*3*8 + 8 = 296 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),              # 16 params\n",
        "            nn.Conv2d(8, 8, 3, padding=1),  # 8*3*3*8 + 8 = 584 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),              # 16 params\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 12, 3, padding=1), # 8*3*3*12 + 12 = 876 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(12),             # 24 params\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        \n",
        "        # 12 channels * 3 * 3 = 108 neurons after three max pools (28->14->7->3)\n",
        "        self.fc1 = nn.Linear(12 * 3 * 3, 10)  # 108*10 + 10 = 1090 params\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 12 * 3 * 3)  # Flatten\n",
        "        x = F.dropout(x, p=0.1)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xdydjYTZFyi3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Net                                      [1, 10]                   --\n",
              "├─Sequential: 1-1                        [1, 4, 14, 14]            --\n",
              "│    └─Conv2d: 2-1                       [1, 4, 28, 28]            40\n",
              "│    └─ReLU: 2-2                         [1, 4, 28, 28]            --\n",
              "│    └─BatchNorm2d: 2-3                  [1, 4, 28, 28]            8\n",
              "│    └─Conv2d: 2-4                       [1, 4, 28, 28]            148\n",
              "│    └─ReLU: 2-5                         [1, 4, 28, 28]            --\n",
              "│    └─BatchNorm2d: 2-6                  [1, 4, 28, 28]            8\n",
              "│    └─MaxPool2d: 2-7                    [1, 4, 14, 14]            --\n",
              "├─Sequential: 1-2                        [1, 8, 7, 7]              --\n",
              "│    └─Conv2d: 2-8                       [1, 8, 14, 14]            296\n",
              "│    └─ReLU: 2-9                         [1, 8, 14, 14]            --\n",
              "│    └─BatchNorm2d: 2-10                 [1, 8, 14, 14]            16\n",
              "│    └─Conv2d: 2-11                      [1, 8, 14, 14]            584\n",
              "│    └─ReLU: 2-12                        [1, 8, 14, 14]            --\n",
              "│    └─BatchNorm2d: 2-13                 [1, 8, 14, 14]            16\n",
              "│    └─MaxPool2d: 2-14                   [1, 8, 7, 7]              --\n",
              "│    └─Dropout: 2-15                     [1, 8, 7, 7]              --\n",
              "├─Sequential: 1-3                        [1, 12, 3, 3]             --\n",
              "│    └─Conv2d: 2-16                      [1, 12, 7, 7]             876\n",
              "│    └─ReLU: 2-17                        [1, 12, 7, 7]             --\n",
              "│    └─BatchNorm2d: 2-18                 [1, 12, 7, 7]             24\n",
              "│    └─MaxPool2d: 2-19                   [1, 12, 3, 3]             --\n",
              "│    └─Dropout: 2-20                     [1, 12, 3, 3]             --\n",
              "├─Linear: 1-4                            [1, 10]                   1,090\n",
              "==========================================================================================\n",
              "Total params: 3,106\n",
              "Trainable params: 3,106\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.36\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.16\n",
              "Params size (MB): 0.01\n",
              "Estimated Total Size (MB): 0.18\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "# Create a dummy input tensor on the correct device\n",
        "summary(model, input_size=(1, 1, 28, 28), device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/dhairyashil/miniconda3/envs/erav2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/Users/dhairyashil/miniconda3/envs/erav2/lib/python3.12/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "torch.manual_seed(1456)\n",
        "batch_size = 512\n",
        "\n",
        "kwargs = {'num_workers': 0, 'pin_memory': True} if device.type in [\"cuda\", \"mps\"] else {}\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=True, download=True,\n",
        "#                     transform=transforms.Compose([\n",
        "#                         transforms.ToTensor(),\n",
        "#                         transforms.Normalize((0.1307,), (0.3081,))\n",
        "#                     ])),\n",
        "#     batch_size=batch_size, shuffle=True, **kwargs)\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "#                         transforms.ToTensor(),\n",
        "#                         transforms.Normalize((0.1307,), (0.3081,))\n",
        "#                     ])),\n",
        "#     batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "# Define the augmentation pipeline\n",
        "train_transforms = A.Compose([\n",
        "    A.ShiftScaleRotate(\n",
        "        shift_limit=0.0625,\n",
        "        scale_limit=0.1,\n",
        "        rotate_limit=15,\n",
        "        p=0.7,\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        value=0\n",
        "    ),\n",
        "    A.GridDistortion(num_steps=5, distort_limit=0.2, p=0.3),\n",
        "    A.GaussNoise(var_limit=(5.0, 30.0), p=0.3),\n",
        "    A.Perspective(scale=(0.05, 0.1), p=0.3, keep_size=True, pad_mode=cv2.BORDER_CONSTANT, pad_val=0),\n",
        "\n",
        "    A.ElasticTransform(\n",
        "         alpha=1.0,\n",
        "         sigma=10.0,\n",
        "         alpha_affine=None,  # Set to None as required by newer versions\n",
        "         interpolation=cv2.INTER_LINEAR,\n",
        "         border_mode=cv2.BORDER_CONSTANT,\n",
        "         value=0,\n",
        "         p=0.3\n",
        "    ),\n",
        "    \n",
        "    # CoarseDropout as alternative to regular dropout\n",
        "    A.CoarseDropout(\n",
        "        max_holes=2,\n",
        "        max_height=8,\n",
        "        max_width=8,\n",
        "        min_holes=1,\n",
        "        fill_value=0,\n",
        "        p=0.2\n",
        "    ),\n",
        "\n",
        "    A.Normalize(\n",
        "        mean=[0.1307],\n",
        "        std=[0.3081],\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# Custom Dataset class to work with Albumentations\n",
        "class MNISTAlbumentations(datasets.MNIST):\n",
        "    def __init__(self, root, train=True, download=True, transform=None):\n",
        "        super().__init__(root, train=train, download=download, transform=None)\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.data[idx], self.targets[idx]\n",
        "        \n",
        "        # Convert to numpy array and add channel dimension\n",
        "        img = np.array(img)\n",
        "        img = np.expand_dims(img, axis=-1)  # Add channel dimension for Albumentations\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            transformed = self.transform(image=img)\n",
        "            img = transformed[\"image\"]\n",
        "            \n",
        "        return img, label\n",
        "\n",
        "\n",
        "# Update the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    MNISTAlbumentations('../data', train=True, download=True, transform=train_transforms),\n",
        "    batch_size=batch_size, shuffle=True, \n",
        "    **kwargs)\n",
        "\n",
        "# Test transforms (only normalization, no augmentation)\n",
        "test_transforms = A.Compose([\n",
        "    A.Normalize(\n",
        "        mean=[0.1307],\n",
        "        std=[0.3081],\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    MNISTAlbumentations('../data', train=False, transform=test_transforms),\n",
        "    batch_size=batch_size, shuffle=True,  \n",
        "    **kwargs)\n",
        "\n",
        "# Optional: Visualization function to check augmentations\n",
        "def visualize_augmentations(dataset, idx=0, samples=5):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for i in range(samples):\n",
        "        data = dataset[idx][0]\n",
        "        if isinstance(data, torch.Tensor):\n",
        "            data = data.numpy()\n",
        "        if data.shape[0] == 1:  # If channels first, move to last\n",
        "            data = np.transpose(data, (1, 2, 0))\n",
        "        plt.subplot(1, samples, i + 1)\n",
        "        plt.imshow(data.squeeze(), cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAB7oAAAGGCAYAAADl+o4QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAw/0lEQVR4nO3be7BdZXn48Wefa04uhpAgBDAESOSiFVQUsFoqolSLbbX0gm2Zjrdidaa21al2Wjt2vHVEdJBqO3ZKq6NTO7RVcRxLHRRqURQvKRCNKIRLiEQgJ+Ryci777N8fnelFaX95HnNWzhs+n7/Pd79rr7322muv5+zeYDAYBAAAAAAAAAA0YuhQbwAAAAAAAAAAZBh0AwAAAAAAANAUg24AAAAAAAAAmmLQDQAAAAAAAEBTDLoBAAAAAAAAaIpBNwAAAAAAAABNMegGAAAAAAAAoCkG3QAAAAAAAAA0xaAbAAAAAAAAgKaMHOgf9nq9hdyOH1tX2zcYDDpZp6qyHxb7c6pY7MdrxdBQ/v9SKvuh3++nmy5VnlNXx8P8/Hyp6+o92OV7/XB8DwI8lvjMAOBA+cwA4EAt9s+MLu+tL+b7+KOjo+lmdnZ2Abbk4KncW6/e6z3cjI+Pl7rp6emDvCWPbmxsLN3MzMwswJY8usp7vfIerMx1FvsxfqDb5xfdAAAAAAAAADTFoBsAAAAAAACAphh0AwAAAAAAANAUg24AAAAAAAAAmmLQDQAAAAAAAEBTDLoBAAAAAAAAaIpBNwAAAAAAAABNMegGAAAAAAAAoCkG3QAAAAAAAAA0xaAbAAAAAAAAgKYYdAMAAAAAAADQFINuAAAAAAAAAJoycqg34If1er3O1hoMBp2ttZgNDeX/36HSjI6OppuIiOHh4U7WGh8fTzeV/VC1ZMmSdPP4xz++k3VmZmbSza5du9JNRMTExES6OeGEE9LNUUcdlW7279+fbiIi7r777nTz7W9/u7QWAAC0YuXKlemm+j0DADi8LF26NN3s27dvAbbk0VXmIJV5xshIfgQ0Pz+fbrq8T17ZvkrDf5ibmyt1lWOi8jrNzs6mm4rKnCoiot/vp5vKvKWyvyuzoIiIqampUrdQ/KIbAAAAAAAAgKYYdAMAAAAAAADQFINuAAAAAAAAAJpi0A0AAAAAAABAUwy6AQAAAAAAAGiKQTcAAAAAAAAATTHoBgAAAAAAAKApBt0AAAAAAAAANMWgGwAAAAAAAICmGHQDAAAAAAAA0BSDbgAAAAAAAACaYtANAAAAAAAAQFNGFvLBe71eJ02XBoNBuqk8p5GR2kuzdOnSdLN8+fJ0s2zZsnSzatWqdBNR275Ks3LlynRT2Q9VlbVOOOGEdDMxMZFu9u3bl262b9+ebiJqr+3JJ5+cblavXp1uqs/ppptuSjf33ntvaS0AAA5PRx11VKk7//zz083P/MzPpJujjz463Vx77bXp5oMf/GC6AQAOP/1+P90MDw+nm7GxsXQTETE1NZVuKnOGubm5dFMxPj5e6qanp9NN5f7wnj170s3QUO13ovPz86WuC5W5U/UYqsyqKnOGysyuonJO6VLluJuZmVmALXl0o6OjC/bYftENAAAAAAAAQFMMugEAAAAAAABoikE3AAAAAAAAAE0x6AYAAAAAAACgKQbdAAAAAAAAADTFoBsAAAAAAACAphh0AwAAAAAAANAUg24AAAAAAAAAmmLQDQAAAAAAAEBTDLoBAAAAAAAAaIpBNwAAAAAAAABNMegGAAAAAAAAoCkG3QAAAAAAAAA0ZWQhH7zX6y3kwx8Slec0OjqablavXp1uIiI2btyYbp74xCemm1WrVqWbNWvWpJuIiMc97nHpZvny5emmss9XrFiRbqrGxsbSzVFHHZVuKsf4XXfdlW7Gx8fTTVXlPbh79+50s2PHjnQTEfHwww+nm6mpqdJaAAD8h8q1cuX71hlnnJFuJicn003l+0JExBve8IZ0c/LJJ6eb2267Ld1s3bo13SxdujTdRETs27ev1AFtqd6r7Op+YPVcnjUYDDrrKvfOZmdn001F5fM2IqLf7x/cDWFBTE9Pd7JO9R7dyEh+NDM3N1daK2toKP/7yK72d8Tiv26rfGZUmvn5+XRTOYYqx2pEbfsWs8r7oqqrfdfl59lCruUX3QAAAAAAAAA0xaAbAAAAAAAAgKYYdAMAAAAAAADQFINuAAAAAAAAAJpi0A0AAAAAAABAUwy6AQAAAAAAAGiKQTcAAAAAAAAATTHoBgAAAAAAAKApBt0AAAAAAAAANMWgGwAAAAAAAICmGHQDAAAAAAAA0BSDbgAAAAAAAACaMrKQDz4YDDppIiJ6vV4na1XWGR0dTTerV69ONxERz3zmM9PNi1/84nSzYsWKdLN06dJ0ExGxbNmydLNkyZJOmoq9e/eWusrxWtnn+/fvTzfbtm1LN9/85jfTTUTE5ORkqcvas2dPutm+fXtprR07dqSbBx98sLQWAEBF5XvQ+Ph4uhkZyX9FrTQRERdffHG6Of/889PNCSeckG6mpqbSTeU6PqL2fevaa69NN9dff326+drXvpZu9u3bl27gsaByHn/c4x5XWmvt2rXppnKurDjttNNK3cTERLo58cQT083JJ5+cbip+8IMflLqZmZl0c8kll6Sbz372s+mm4jWveU2pu++++w7ylrBYjI2NpZvK+yKidg07Pz+fbirn/36/n24q+y4iYm5uLt1U5i2V51TZtqqhofxvUoeHh9NN5Riq7O+IiNnZ2XRTmdFU3oOVfVf53hRRm+tUvttV9nfleKhayLX8ohsAAAAAAACAphh0AwAAAAAAANAUg24AAAAAAAAAmmLQDQAAAAAAAEBTDLoBAAAAAAAAaIpBNwAAAAAAAABNMegGAAAAAAAAoCkG3QAAAAAAAAA0xaAbAAAAAAAAgKYYdAMAAAAAAADQFINuAAAAAAAAAJpi0A0AAAAAAABAU0YW8sEHg0G66fV6i3qtSjM7O5tu9uzZk24iIvbu3ZtuVq5cmW5WrVqVbqqv7erVq9PNxMREuqns83vuuSfdfOc730k3ERFzc3Pp5vGPf3y6WbJkSbq59dZb083nP//5dBMRsX379nTT7/fTTWV/V9aprgU8uuHh4XRT+Rzsyute97pSt3Tp0nRzyimnpJvXvva16ebyyy9PN5dcckm6iYjYv39/unnXu96Vbt761remG2hN5bryggsuSDfPfvaz080znvGMdBMR8fSnP73UZe3cuTPdfOhDH0o3mzdvTjcRETt27Eg39957b7p54IEH0s3k5GS6AQCompmZSTcjI7URy/T0dLqpzEC6Utl3EbWZQWWtoaH8bz5HR0fTTdWaNWs6WyvrsssuK3UrVqxIN+vWrUs3f/AHf5Bu3vnOd6abiy++ON1ERDzyyCPp5o//+I/Tzfvf//5006XKe/CAH3vBHhkAAAAAAAAAFoBBNwAAAAAAAABNMegGAAAAAAAAoCkG3QAAAAAAAAA0xaAbAAAAAAAAgKYYdAMAAAAAAADQFINuAAAAAAAAAJpi0A0AAAAAAABAUwy6AQAAAAAAAGiKQTcAAAAAAAAATTHoBgAAAAAAAKApBt0AAAAAAAAANGVkIR+81+st5MMfEoPBIN3Mzc2lm507d6abiIivfOUr6WblypXp5qijjko3Y2Nj6SYi4txzz003GzZsSDc/+MEP0s2//du/pZvrr78+3URETE5Opps1a9akm2OPPTbd3H777enm4YcfTjcREXv27Ek3lfdtpanqci3479atW5duKufyZz3rWenm2c9+drqJiDjiiCPSzS/+4i+W1jrc3HfffenmyiuvTDcveclL0s3u3bvTTUTEpk2b0s0NN9xQWgv+u5GR/NesiYmJdLNx48Z085znPCfdREScffbZnax1/PHHp5suVa7bli9fnm4++9nPppt///d/TzcREQ899FCpA9pSOY9fffXVpbUq95mWLVtWWqurdYaHhw/yljy6+fn5Ttb53d/93VI3Ozubbj796U+nm8p9port27d3sg7tqLzXq+/b8fHxdFOZM1S2r9IsXbo03URErF+/Pt1U5k6Ve1PnnHNOuomofQ6+9KUvTTeV42F0dDTdVM791bXuvPPOdPPud7873Zx//vnppvq96etf/3q6+epXv1paqytDQ/nfUC/kNY5fdAMAAAAAAADQFINuAAAAAAAAAJpi0A0AAAAAAABAUwy6AQAAAAAAAGiKQTcAAAAAAAAATTHoBgAAAAAAAKApBt0AAAAAAAAANMWgGwAAAAAAAICmGHQDAAAAAAAA0BSDbgAAAAAAAACaYtANAAAAAAAAQFMMugEAAAAAAABoyshCPvhgMEg3vV5vAbbk0Krsh71795bWuvPOO9PNZz7zmXSzYsWKTpqIiNHR0XSzfPnydLNr1650c9ddd6Wb2267Ld1ERGzfvj3dTExMpJsjjjgi3ezZsyfd7N69O91ERPT7/VKXVTkXVd7rVYfjuZK6M888s9Rdf/316WblypWltejW/Px8uvmjP/qjdFM5/3/0ox9NN5XPwIiInTt3ppstW7aU1uLwVfnMPf7449PNueeem24uvPDCdHPOOeekm4iIdevWpZvKtejc3Fy6ue+++9JNRMQDDzyQbk477bR0s2PHjnRT+Q700EMPpRvgseP+++9PN9XvuNPT0+lm7dq1pbWy9u/fX+r+5m/+Jt2sWbMm3TzlKU9JNxVXXnllJ+tAa7q67xgRMTMz08k6lfsDFU984hNL3Y033phuqnOGrOrxMDw8nG5mZ2fTTeVzempqKt0MDdV+L1v5bvfmN7853VT23Yc+9KF0U/2+VZk7LfZ7U12dVw6UX3QDAAAAAAAA0BSDbgAAAAAAAACaYtANAAAAAAAAQFMMugEAAAAAAABoikE3AAAAAAAAAE0x6AYAAAAAAACgKQbdAAAAAAAAADTFoBsAAAAAAACAphh0AwAAAAAAANAUg24AAAAAAAAAmmLQDQAAAAAAAEBTDLoBAAAAAAAAaIpBNwAAAAAAAABNGTnUG3Cw9Hq9dDMYDBZgS35UZdv6/X5prd27d6ebO++8M90MDw+nm+XLl6ebiIgTTjihk6ayfUuXLk03VXv27Ek3k5OT6ebBBx9MN5X30tzcXLo5XFXOEfDf3XPPPaXuoYceSjcrV64srXW4ufnmm9NN5Zz83Oc+N91ERMzMzKSbj3zkI6W14HB37LHHppvLL7883Vx00UXpZnx8PN0sdvv37083V199dWmt66+/Pt1Uvm+dfvrp6eaRRx5JNwD/l23btqWbSy+9tLRW5Vz+iU98orRW1pe+9KVSd9lll6Wb0dHRdPPUpz413QBtmp+fP9Sb8L+q3KusfM5ERNx7773pZt26delmZCQ/Chsaqv1OtLL/KvOWL3/5y+nm+9//frq54IIL0k1E7Rj/+7//+3SzZMmSdFO5VlnsFvOMdKH5RTcAAAAAAAAATTHoBgAAAAAAAKApBt0AAAAAAAAANMWgGwAAAAAAAICmGHQDAAAAAAAA0BSDbgAAAAAAAACaYtANAAAAAAAAQFMMugEAAAAAAABoikE3AAAAAAAAAE0x6AYAAAAAAACgKQbdAAAAAAAAADTFoBsAAAAAAACApowc6g04WHq93qHehP9VZdsGg0FprX6/n26mpqbSzfz8fLqZnp5ONxER3/zmN9PNunXr0s1ZZ52VbtavX59unvzkJ6ebiNrr9NBDD6Wbyus0MzOTbqrHeFcq21c9D1W6ynuQw9fDDz9c6t74xjemm4suuijdfOMb30g3V155ZbqpqnzOPP/5z083e/fuTTdPetKT0k1ExO/8zu+UOuBHVa7BKueVZz3rWemmcu2/devWdBMR8exnP7vUZd1xxx3p5rrrriut9ZWvfCXdDA3l/1d806ZN6QbgYKt8Ztxyyy0LsCWPrvr5lHXqqaeWusp38Mo1xE033ZRugDZVriu7uh9YuS86OTlZWutNb3pTunnBC16QbjZv3pxurrjiinQTUXttK99NLrjggnRT+WzasGFDuomIeMMb3lDqsirXOF0aHR1NN7Ozs+lmsc9bFpJfdAMAAAAAAADQFINuAAAAAAAAAJpi0A0AAAAAAABAUwy6AQAAAAAAAGiKQTcAAAAAAAAATTHoBgAAAAAAAKApBt0AAAAAAAAANMWgGwAAAAAAAICmGHQDAAAAAAAA0BSDbgAAAAAAAACaYtANAAAAAAAAQFMMugEAAAAAAABoysiB/mGv11vI7fhPg8Ggk3W6Xiurur/n5+c7aSr7bnZ2Nt1ERHznO99JN1/4whfSzYoVK9LNqaeemm6e+9znppuIiJUrV6abu+66K93cfffd6ea+++5LN1NTU+mmS5VjvMtzSlfnZA5vn/jEJ9LN9ddfn252796dbs4444x0ExHxile8It1cfvnl6Wbv3r3ppuL2228vda9+9asP8pbAY9euXbvSzd/93d+lmwceeCDdrF+/Pt3ceOON6SYi4jd/8zfTzXnnnZduKue9kZED/lr7P1Su3ebm5kprAQDQrco1YpfXepVr0cr9wOHh4XRTUZkxRER85jOfSTeVe/+Ve1PnnHNOuomIuPTSS9PN+973vnQzMzOTbiq+973vlbrLLrvsIG/Jo6vOnboyNjaWbrp6TpVti1h8s1W/6AYAAAAAAACgKQbdAAAAAAAAADTFoBsAAAAAAACAphh0AwAAAAAAANAUg24AAAAAAAAAmmLQDQAAAAAAAEBTDLoBAAAAAAAAaIpBNwAAAAAAAABNMegGAAAAAAAAoCkG3QAAAAAAAAA0xaAbAAAAAAAAgKYYdAMAAAAAAADQlJGFfPBer7eQD/8/DAaDztZazCr7vKt91+/3S90jjzySbjZt2pRuli9fnm6WLFmSbk4//fR0U+0efPDBdHPzzTenm+uuuy7d3HXXXekmImLv3r3pZm5urrRWVyrvweHh4QXYEvj/q5yTK3bt2tXJOhERr3rVq9LNxz/+8XQzPz+fboBDr3INe+edd6abHTt2pJvKtk1PT6ebiIhVq1alm3Xr1qWbCy64IN1MTU2lm4iI7373u+nm+9//fmmtrMX8vQ5gIbztbW/rZJ2PfvSjpe7Vr351uvnc5z6Xbqr3SrKq9+igJYv9fmDl3l7lvbvY90PF7t27O1ln+/btpa7yOv32b/92uqncm6oYGqr9Xrby/WQx3zsbGamNUyvzjK5UZwyzs7PpZiHPRX7RDQAAAAAAAEBTDLoBAAAAAAAAaIpBNwAAAAAAAABNMegGAAAAAAAAoCkG3QAAAAAAAAA0xaAbAAAAAAAAgKYYdAMAAAAAAADQFINuAAAAAAAAAJpi0A0AAAAAAABAUwy6AQAAAAAAAGiKQTcAAAAAAAAATTHoBgAAAAAAAKApBt0AAAAAAAAANKU3GAwGB/KHw8PDC70tERFxgJtzULper1daq4t1qtvW7/dLXReGhrr7v4rR0dF0s3bt2nRz5plnpptzzz033UREnHXWWenmxBNPTDc7d+5MN5///OfTzQ033JBuIiJuvfXWdLNjx450Mz09nW66PH9VzM/Pd7JORHfnVw5fy5YtK3XXXnttujnvvPPSzQtf+MJ0c91116UbOFS6+myK8Jnx46jsu+pre9JJJ6WbCy64IN289rWvTTennnpquomI+NM//dN0s3Xr1nQzOzubbjZv3pxu7r777nQTUbvunZmZKa3F4clnBgfD2NhYJ+u8/vWvL3W///u/n262bduWbt7+9renm4rPfvazpW7v3r0HeUt4rOnyM6NyL7rL7avo8vp/Mat8ZnR5/Vq5/1OZGbzsZS9LN5Xzf/UYmpubSzddzZC6PD9U3reVfdfl+2J8fDzdLOS8xS+6AQAAAAAAAGiKQTcAAAAAAAAATTHoBgAAAAAAAKApBt0AAAAAAAAANMWgGwAAAAAAAICmGHQDAAAAAAAA0BSDbgAAAAAAAACaYtANAAAAAAAAQFMMugEAAAAAAABoikE3AAAAAAAAAE0x6AYAAAAAAACgKQbdAAAAAAAAADSlNxgMBgfyh0NDi3smfoBP45Do9XqdNBER8/Pzpa4L1edU7bLGx8fTzZo1a9LNhg0b0k1ExFlnnZVunvnMZ6abyvZVjrtvfvOb6SYi4sYbb0w3X/7yl9PN1q1b083MzEy6iaidvyrviy7PD129b+GHnXzyyenm61//erqZnJxMN5///OfTzS233JJuIiL+/M//PN0s5msputfl8eAzow2V12nJkiXp5rzzzks373znO9NNRMTq1avTze7du9NN5Rrx+9//frr55Cc/mW4iap81W7ZsSTd79+5NN4v5+y3/xWcGLRkbGyt1L37xi9PN+973vnRzxBFHpJuKN73pTaXur//6r9PN1NRUaS0OT11+ZlTe77Ozs+lmeHg43URE9Pv9UpdV2b6utq2qcj3Q1f3XiIgTTzwx3Xz1q19NN5XvJpV769/4xjfSTUTt3lTlPViZXXZ5jHd1vC52IyMj6eZAj4fFPb0GAAAAAAAAgB9i0A0AAAAAAABAUwy6AQAAAAAAAGiKQTcAAAAAAAAATTHoBgAAAAAAAKApBt0AAAAAAAAANMWgGwAAAAAAAICmGHQDAAAAAAAA0BSDbgAAAAAAAACaYtANAAAAAAAAQFMMugEAAAAAAABoikE3AAAAAAAAAE3pDQaDwQH9Ya+Xf/BCU3WAT+OQqOyHxfx8qqrHQ1f7r7LOyMhIulm+fHm6iYg49thj083pp5+ebi644IJ0c+GFF6ab2dnZdBMRsWnTpnRzzTXXpJvrrrsu3ezcuTPdRETMz8+nm8rxWlmnqsvzP/y4XvKSl6Sbq6++Ot2sWLEi3VT94R/+Ybr58Ic/nG62b9+ebmhDl9eiPjP47yYmJtLNOeecU1rrpS99abo599xz003lOn7NmjXppt/vp5uIiM997nPp5l/+5V86WWfz5s3phu75zOCxYGgo/1uhyj2ZG264Id1UDA8Pl7pXvvKV6aZyT4bDV5efGZX7tpV7Z4v9Oc3NzaWbyjX59PR0uono7r7oYp+3XHzxxenmAx/4QLpZvXp1uql+z3jLW96Sbj7xiU+kmy1btqSbyudg9d56V8d45fxQndF09R480MYvugEAAAAAAABoikE3AAAAAAAAAE0x6AYAAAAAAACgKQbdAAAAAAAAADTFoBsAAAAAAACAphh0AwAAAAAAANAUg24AAAAAAAAAmmLQDQAAAAAAAEBTDLoBAAAAAAAAaIpBNwAAAAAAAABNMegGAAAAAAAAoCkG3QAAAAAAAAA0ZeRQb8DB0uv1DvUm/K8W87ZVDQaDdFPdD5Wusn2VZn5+Pt3s3bs33URE3H///elmZmYm3axevTrdPOc5z0k3J554YrqJiFi2bFm62bp1a7r5+te/nm4eeeSRdBNRO44qxyvw6P7pn/4p3dxxxx3p5oorrkg3z3ve89JNRMQ73vGOdHPCCSekm7e//e3pZtu2bekGeOyYmppKN1/+8pdLa23atCndnHTSSenmhS98Ybo57rjjOlknIuKiiy5KN2eeeWa6Of7449PNBz7wgXRTufYHAA4//X4/3QwN5X8XWGkiatvX1f3AyjX52NhYaa3K/evx8fF0Mz09nW66nGdcc8016aZyb6pyv+j5z39+uomIeNvb3pZuNmzY0Mk69913X7rp8n58V/Ot4eHhdBNRO38tJL/oBgAAAAAAAKApBt0AAAAAAAAANMWgGwAAAAAAAICmGHQDAAAAAAAA0BSDbgAAAAAAAACaYtANAAAAAAAAQFMMugEAAAAAAABoikE3AAAAAAAAAE0x6AYAAAAAAACgKQbdAAAAAAAAADTFoBsAAAAAAACAphh0AwAAAAAAANCUkYV88MFgsJAP34xer5duhoZq/4MwPz/fyVr9fj/dVI+HrvZfZZ2xsbF0c8QRR6SbiIhjjz023WzYsCHdHHPMMelmeHg43VQtWbIk3axataqTZnR0NN1ERMzNzaUb51c4tG677bZ088u//Mvp5sUvfnG6iYi4+uqr081v/dZvpZuNGzemm+c///npBuD/MjU11Vn38MMPp5vbb7893axduzbdVL7PRES88pWvTDeV7wxnn312unnve9+bbgAWQuW8VzmXd2ViYqLUHXfccQd5S2Bxqdxb71LlnnxXZmZmSt3ISH5EtX///k7Wqd5/rRxHle3btGlTunnZy16Wbl70ohelm4iIj33sY+nmFa94RbpZt25dunnhC1+YbqrHQ+W1rcwLKs3hwi+6AQAAAAAAAGiKQTcAAAAAAAAATTHoBgAAAAAAAKApBt0AAAAAAAAANMWgGwAAAAAAAICmGHQDAAAAAAAA0BSDbgAAAAAAAACaYtANAAAAAAAAQFMMugEAAAAAAABoikE3AAAAAAAAAE0x6AYAAAAAAACgKQbdAAAAAAAAADTFoBsAAAAAAACApowc6g34Yb1er9NusZqfny91Q0P5/12orpVVfY0Gg0G6GR8fTzdr1qxJNxs2bEg3T37yk9NNRMTGjRvTzSmnnJJujj766HSzevXqdDM1NZVuIiLuv//+dLNt27Z0Mzk5mW5mZmbSTUTtGAfaUzmvfOQjHymt9Vd/9VfpZmQkf1n4Uz/1U+nmp3/6p9PNF77whXQDsFhMTEykmyc84QnpZunSpekmovYdcm5uLt1U9gPw2FE9h2VdeOGFpe7SSy9NN2eccUa6OeKII9JNxc6dO0vdli1bDvKWwOJS+V7c7/dLa3V1P7Cr51R9PpWuq+vX4eHhdBNRe05dXV/v27cv3fzDP/xDuomo3SuvvLbPe97z0s25556bbm666aZ0E1F7bSv7oXLcdTmXqDynA37sBXtkAAAAAAAAAFgABt0AAAAAAAAANMWgGwAAAAAAAICmGHQDAAAAAAAA0BSDbgAAAAAAAACaYtANAAAAAAAAQFMMugEAAAAAAABoikE3AAAAAAAAAE0x6AYAAAAAAACgKQbdAAAAAAAAADTFoBsAAAAAAACAphh0AwAAAAAAANCUkUO9AT9sMBgc6k1YFKr7YX5+vpO1er1eupmYmEg3ERFHHnlkujn55JPTzU/8xE+km6c85Snp5pRTTkk3ERFr165NNytXrkw3/X4/3ezatSvdbN++Pd1ERHzrW99KN5s3b043e/bsSTddnr8q70Hg4Kmc/y+++OJ084xnPCPdRESMjHRziVc5v954440LsCUA3ah8N7nwwgvTza/8yq+kmzPPPDPdVA0PD6ebLVu2pJtly5alG+DQW7NmTbp585vfvABb8qN+/dd/vdStXr063VTOlZV7HhVvetObSt0NN9xwkLcEFk7l3tnc3Fy6qbzXI2r3YCsqz2loKP/7yEoTUdsP4+Pj6aayH6qvUeXYW7JkSbp50pOelG4q96aq3zPGxsbSTWW+Vbk39cUvfjHddKlyDFVmE9X3beV1qp4rD4RfdAMAAAAAAADQFINuAAAAAAAAAJpi0A0AAAAAAABAUwy6AQAAAAAAAGiKQTcAAAAAAAAATTHoBgAAAAAAAKApBt0AAAAAAAAANMWgGwAAAAAAAICmGHQDAAAAAAAA0BSDbgAAAAAAAACaYtANAAAAAAAAQFMMugEAAAAAAABoysiB/mGv11vI7TgkDsfnNDSU/9+F0dHRdLNixYp0s3HjxnQTEfGkJz0p3TzlKU9JN6eddlq6OeGEE9LNypUr001E7bXds2dPurn//vvTzfe+9710s3nz5nRT7bZs2ZJuJicn0838/Hy6AQ6uU045Jd287nWvSzcvfelL080xxxyTbrrU7/fTzfbt29ONcyVwsI2NjZW6DRs2pJtLLrkk3bz85S9PN8cee2y6qapc/3/0ox9NNx/4wAfSzdatW9MNtKare1OrVq1KN5V7KxERH/zgB9PNqaeeWlora25urtTt2LEj3VT2w+WXX55uKqampjpZBw6lwWCQbir3ySv3bCNq340rz6myfZVtq16TV+5FVJrKcxoZOeDx2f9QmRn83u/9Xrr5uZ/7uXRz/PHHp5vqfZzKZ25lnlG5919Rfa9XVI7xiso5ZTGu5RfdAAAAAAAAADTFoBsAAAAAAACAphh0AwAAAAAAANAUg24AAAAAAAAAmmLQDQAAAAAAAEBTDLoBAAAAAAAAaIpBNwAAAAAAAABNMegGAAAAAAAAoCkG3QAAAAAAAAA0xaAbAAAAAAAAgKYYdAMAAAAAAADQFINuAAAAAAAAAJoycqg34GDp9XqdNBXDw8PpZvny5aW1jjnmmHRz5JFHppvjjjsu3TztaU9LNxERZ555Zro5+eST082qVavSTcWuXbtK3QMPPJBu7rzzznTz7W9/u5PmO9/5TrqJiNi2bVu6eeSRR9LN7OxsuhkMBukGHgsqn02XXHJJaa3Xve516Wb9+vWltRazW265Jd28/e1vTzef+tSn0g3AwXbSSSeVuspnzcte9rJ08/jHPz7dzM3NpZvq9fXHPvaxdPPxj3883dxzzz3pBgCgS5VrsKrKfcShofzvFkdG8iOgmZmZdNOlynOq3Jv6tV/7tXQTEfGqV70q3VTmGRV79uxJN2NjY6W1vva1r6Wbd7zjHenm05/+dLqpGB8fL3VTU1MHeUsOvcrMcyHnsX7RDQAAAAAAAEBTDLoBAAAAAAAAaIpBNwAAAAAAAABNMegGAAAAAAAAoCkG3QAAAAAAAAA0xaAbAAAAAAAAgKYYdAMAAAAAAADQFINuAAAAAAAAAJpi0A0AAAAAAABAUwy6AQAAAAAAAGiKQTcAAAAAAAAATTHoBgAAAAAAAKApIwf6h4PBYCG348fW6/XSzejoaLqZmJhINytXrkw3GzduTDcREc95znPSzYknnphunvCEJ6SbtWvXppuIiKOPPjrdjI+Pp5u9e/emm3vvvTfdfOtb30o3ERG33357utm8eXO6ueOOO9JNZT/s2bMn3UREzM/Pl7qsLs95lfNXpYEfVjm/nn766enmqquuSjennnpqulnsbr755nTz7ne/u7TWJz/5yXTT1fkVeOwYGTngr5v/af369enm0ksvTTcREb/xG7+Rbo499th0MzSU///yrVu3ppu//du/TTcRER//+MfTzT333JNuFvs9BTgYKueIM844YwG25Ee99a1vTTennXZaaa3ly5enm507d5bWyvrHf/zHUvf+978/3dx2223ppt/vpxvg0VWuwSrN3NxcuqmqXE/NzMykm8oMpLofjjrqqHRz9tlnp5vK/ZXqjGZ4eDjd7N+/v5N1vva1r6Wb9773vekmIuIzn/lMupmdnS2t1YWpqalS19V9/Mr5q3rdsdi+2/lFNwAAAAAAAABNMegGAAAAAAAAoCkG3QAAAAAAAAA0xaAbAAAAAAAAgKYYdAMAAAAAAADQFINuAAAAAAAAAJpi0A0AAAAAAABAUwy6AQAAAAAAAGiKQTcAAAAAAAAATTHoBgAAAAAAAKApBt0AAAAAAAAANMWgGwAAAAAAAICmGHQDAAAAAAAA0JSRA/3D4eHh9IPPz8+nm8FgkG4iIsbHx9PNunXr0s0ZZ5yRbp72tKelm9NPPz3dRESccsop6WbNmjXpZmJiIt30er10E1E7jh588MF0s2XLlnRzyy23pJubb7453URE3Hrrrelm+/bt6WZ6ejrd9Pv9dFN9rw8N5f8/p3LsVbav+py6XIvF78gjj0w3f/mXf1la68wzz0w3J510Ummtxeymm25KN+95z3vSzT//8z+nm6mpqXQD8H+pXpMvXbo03axfvz7dvOUtb0k3v/ALv5BuIiLGxsbSzeTkZLr54he/mG4qn+3XXXdduomImJmZKXXAj3rNa16Tbi677LIF2JIfVfmecdttt5XW2rFjR7r58Ic/XFor69prry11lfM/cGhV7ilXmqrKdXmlGR0dTTcrV65MN3/xF3+RbiIiTjvttHSzcePGdFO5lzo3N5duImr3yiv3pq666qp086lPfSrdVJ5PVWUOWdm+yvui+l268n1ryZIl6aYy16labOdXv+gGAAAAAAAAoCkG3QAAAAAAAAA0xaAbAAAAAAAAgKYYdAMAAAAAAADQFINuAAAAAAAAAJpi0A0AAAAAAABAUwy6AQAAAAAAAGiKQTcAAAAAAAAATTHoBgAAAAAAAKApBt0AAAAAAAAANMWgGwAAAAAAAICmGHQDAAAAAAAA0JTeYDAYHMgfDg3lZ+IH+NAHxTHHHJNufvInfzLdvOAFL0g3T3/609PN2rVr001ExIoVK9LNnj170s2DDz6Ybnbu3Jluqt0999yTbm699dZOmrvuuivdRERMTk6mm9nZ2XQzPz+fbrrU6/UO9SYcdF2dK7s8Jx+Or9PZZ5+dbt74xjemm2c+85np5rjjjks3i92+fftK3ZVXXplu3vGOd6SbvXv3phtoic+Mw1fle11ExHnnnZdufv7nfz7d/NIv/VK6qXwXjIiYmZlJN9dff326+eAHP5hu/vVf/zXd7Nq1K93AweAz479s2LAh3axevXoBtuTgeOihh0rd9PR0utm2bVtprazFfs8DDnddfmaMjIykm36/vwBb8ujGx8fTzVOf+tR08/rXvz7dnHvuuenm+OOPTzcRtWNiamoq3VT2d2VuEhFx1VVXpZsrrrgi3VTmBRXDw8Olrsv302I2NjaWbipznYouz8kVB7p9ftENAAAAAAAAQFMMugEAAAAAAABoikE3AAAAAAAAAE0x6AYAAAAAAACgKQbdAAAAAAAAADTFoBsAAAAAAACAphh0AwAAAAAAANAUg24AAAAAAAAAmmLQDQAAAAAAAEBTDLoBAAAAAAAAaIpBNwAAAAAAAABNMegGAAAAAAAAoCkjB/qHvV5vIbfjx7Z///50c++996abr371q+lmx44d6WblypXpJiJiYmIi3ezZsyfdPPTQQ500ERE7d+5MN5XXdtu2bemmsm2V/R0RMT8/n24Gg0FpLWq63N+L/Zx8OHrJS17SSdOlzZs3p5tPf/rT6WZubi7dvOc970k3ERGTk5OlDuBQq3y2L1u2LN38yZ/8SbqJiLjooovSzfr169PNkiVL0k2/3083ERFbt25NNx/+8IfTzXXXXZduZmZm0g1w6H33u9/tpAHg/294eDjdVK8rK2ZnZ9NN5T7Ti170onQzOjqabh555JF0ExGxZcuWdFO5vq5417veVeqmpqbSTeW+8tjYWLqpfM/o8n2xmC1durTU7du37yBvyaMbHx9PN9PT0wuwJd3zi24AAAAAAAAAmmLQDQAAAAAAAEBTDLoBAAAAAAAAaIpBNwAAAAAAAABNMegGAAAAAAAAoCkG3QAAAAAAAAA0xaAbAAAAAAAAgKYYdAMAAAAAAADQFINuAAAAAAAAAJpi0A0AAAAAAABAUwy6AQAAAAAAAGiKQTcAAAAAAAAATekNBoPBgfzh8PDwQm9LREQc4Ob8iNHR0XSzYsWKdHPEEUekm4mJiXQzMjKSbiIixsfH083c3Fy62b9/f7qZnp5ONxERU1NT6WbXrl2drDM/P59uqsd4r9frbK3FrLIfFrvK61TZD5XjtepwfJ0AHku6vIY4HD8zKtfkZ511Vrr52Z/92XTzq7/6q+kmImL9+vXp5uGHH043d9xxR7q57bbb0k1ExJ/92Z+lm+9+97ulteBw5jMDgAPV5WfG2NhYupmdnU031fv4lXvyledUuR9Y2bbq/Kjf75e6rC6vIYaG8r8vreyHyj7van9HdDfPWLJkSbqpzLeqx9DhOKPp6tg70H3nF90AAAAAAAAANMWgGwAAAAAAAICmGHQDAAAAAAAA0BSDbgAAAAAAAACaYtANAAAAAAAAQFMMugEAAAAAAABoikE3AAAAAAAAAE0x6AYAAAAAAACgKQbdAAAAAAAAADTFoBsAAAAAAACAphh0AwAAAAAAANAUg24AAAAAAAAAmmLQDQAAAAAAAEBTRg70D3u9XvrBB4NBuqmanZ1NN5OTk+lm165d6aayH6r7rqt9XllnaKj2fxVd7b/KMd6lLt9PXVjs+3uxb9/hdjwAwOFk48aN6eblL395unnBC16Qbo4++uh0ExGxd+/edHPDDTekm2uuuSbd3H333ekmIuKee+4pdQAALH6VeUFFv9/vZJ2I2v3Aubm5BdiSH9XlfhgfH08309PT6WZ0dDTdRNSOvcrspMt9XtHV/ev9+/d3sk71eJiZmTnIW3LoLbZjzy+6AQAAAAAAAGiKQTcAAAAAAAAATTHoBgAAAAAAAKApBt0AAAAAAAAANMWgGwAAAAAAAICmGHQDAAAAAAAA0BSDbgAAAAAAAACaYtANAAAAAAAAQFMMugEAAAAAAABoikE3AAAAAAAAAE0x6AYAAAAAAACgKQbdAAAAAAAAADRl5FBvwMEyGAzSzfz8/AJsyaFV2Q/U9Xq9ztby2v6HLvd51mLeNgCgO094whPSzejoaLr53Oc+l25uvvnmdBMR8aUvfSndbNq0qbQWAAC0ost7trOzs52sMzSU/31kl7OW6enpTtbp9/udrBPR3f4bGcmPBOfm5kprjY2NpZuZmZnSWlmVY7yrbYuIGB4eTjddHq+LjV90AwAAAAAAANAUg24AAAAAAAAAmmLQDQAAAAAAAEBTDLoBAAAAAAAAaIpBNwAAAAAAAABNMegGAAAAAAAAoCkG3QAAAAAAAAA0xaAbAAAAAAAAgKYYdAMAAAAAAADQFINuAAAAAAAAAJpi0A0AAAAAAABAUwy6AQAAAAAAAGhKbzAYDA71RgAAAAAAAADAgfKLbgAAAAAAAACaYtANAAAAAAAAQFMMugEAAAAAAABoikE3AAAAAAAAAE0x6AYAAAAAAACgKQbdAAAAAAAAADTFoBsAAAAAAACAphh0AwAAAAAAANAUg24AAAAAAAAAmvL/ADWeHkThvX7mAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 2000x400 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples in train dataset: 60000\n",
            "Number of samples in test dataset: 10000\n"
          ]
        }
      ],
      "source": [
        "# Uncomment to visualize augmentations\n",
        "visualize_augmentations(train_loader.dataset)\n",
        "\n",
        "# # print number of samples in train and test dataset\n",
        "print(f\"Number of samples in train dataset: {len(train_loader.dataset)}\")\n",
        "print(f\"Number of samples in test dataset: {len(test_loader.dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch, scheduler=None):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MMWbLWO6FuHb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.5153403282165527 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.50it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1595, Accuracy: 9519/10000 (95.19%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.44261348247528076 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1099, Accuracy: 9668/10000 (96.68%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.22794997692108154 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0926, Accuracy: 9709/10000 (97.09%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.3529849052429199 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.70it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0947, Accuracy: 9718/10000 (97.18%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.295000821352005 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.71it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0822, Accuracy: 9754/10000 (97.54%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.37374138832092285 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0662, Accuracy: 9806/10000 (98.06%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.26952099800109863 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0702, Accuracy: 9777/10000 (97.77%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.1477661281824112 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.60it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0733, Accuracy: 9769/10000 (97.69%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.25704240798950195 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0597, Accuracy: 9805/10000 (98.05%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.1488422006368637 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.69it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0600, Accuracy: 9816/10000 (98.16%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.1876668930053711 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.60it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0517, Accuracy: 9838/10000 (98.38%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.22511929273605347 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0620, Accuracy: 9814/10000 (98.14%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.40174010396003723 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0540, Accuracy: 9825/10000 (98.25%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "for epoch in range(1, 14):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current learning rate: 0.040000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.3176345229148865 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.65it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1173, Accuracy: 9640/10000 (96.40%)\n",
            "\n",
            "Current learning rate: 0.082265\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.3383333683013916 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.69it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0905, Accuracy: 9725/10000 (97.25%)\n",
            "\n",
            "Current learning rate: 0.189210\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.13150779902935028 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0722, Accuracy: 9764/10000 (97.64%)\n",
            "\n",
            "Current learning rate: 0.310615\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.22246579825878143 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0623, Accuracy: 9796/10000 (97.96%)\n",
            "\n",
            "Current learning rate: 0.389467\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.22129005193710327 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0523, Accuracy: 9836/10000 (98.36%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0530, Accuracy: 9830/10000 (98.30%)\n",
            "\n",
            "Current learning rate: 0.379960\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.11158696562051773 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0535, Accuracy: 9814/10000 (98.14%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0393, Accuracy: 9869/10000 (98.69%)\n",
            "\n",
            "Current learning rate: 0.352371\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.20513562858104706 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0491, Accuracy: 9852/10000 (98.52%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0386, Accuracy: 9880/10000 (98.80%)\n",
            "\n",
            "Current learning rate: 0.309401\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.2614821791648865 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.68it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0510, Accuracy: 9832/10000 (98.32%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0367, Accuracy: 9879/10000 (98.79%)\n",
            "\n",
            "Current learning rate: 0.255255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.13497264683246613 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0409, Accuracy: 9867/10000 (98.67%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0342, Accuracy: 9892/10000 (98.92%)\n",
            "\n",
            "Current learning rate: 0.195233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.39012646675109863 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0461, Accuracy: 9858/10000 (98.58%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0341, Accuracy: 9895/10000 (98.95%)\n",
            "\n",
            "Current learning rate: 0.135212\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.13719280064105988 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0414, Accuracy: 9878/10000 (98.78%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0335, Accuracy: 9890/10000 (98.90%)\n",
            "\n",
            "Current learning rate: 0.081066\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.17263805866241455 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0405, Accuracy: 9872/10000 (98.72%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0336, Accuracy: 9895/10000 (98.95%)\n",
            "\n",
            "Current learning rate: 0.038095\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.1456877738237381 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.64it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0381, Accuracy: 9880/10000 (98.80%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0347, Accuracy: 9886/10000 (98.86%)\n",
            "\n",
            "Current learning rate: 0.010506\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.1492350548505783 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.61it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0357, Accuracy: 9884/10000 (98.84%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0329, Accuracy: 9904/10000 (99.04%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = Net().to(device)\n",
        "from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "\n",
        "swa_model = AveragedModel(model)\n",
        "\n",
        "# Use OneCycleLR scheduler for better convergence\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.4,\n",
        "    epochs=15,                          # Increased epochs\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=0.3,                      # Warm up for 30% of training\n",
        "    div_factor=10,                      # Initial lr = max_lr/10\n",
        "    final_div_factor=100                # Final lr = initial_lr/100\n",
        ")\n",
        "\n",
        "# Start SWA later in training\n",
        "swa_start = 5\n",
        "swa_scheduler = SWALR(optimizer, swa_lr=0.001)\n",
        "\n",
        "for epoch in range(1, 15):  # Increased to 20 epochs\n",
        "    # Get current learning rate\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f'Current learning rate: {current_lr:.6f}')\n",
        "    \n",
        "    if epoch < swa_start:\n",
        "        train(model, device, train_loader, optimizer, epoch, scheduler)\n",
        "    else:\n",
        "        train(model, device, train_loader, optimizer, epoch)\n",
        "        swa_model.update_parameters(model)\n",
        "        swa_scheduler.step()\n",
        "    \n",
        "    test(model, device, test_loader)\n",
        "    \n",
        "    if epoch >= swa_start:\n",
        "        torch.optim.swa_utils.update_bn(train_loader, swa_model, device=device)\n",
        "        test(swa_model, device, test_loader)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current learning rate: 0.040000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/118 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.3494378626346588 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.65it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1121, Accuracy: 9665/10000 (96.65%)\n",
            "\n",
            "Current learning rate: 0.130463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.24423645436763763 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1120, Accuracy: 9667/10000 (96.67%)\n",
            "\n",
            "Current learning rate: 0.310923\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.30810320377349854 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0741, Accuracy: 9769/10000 (97.69%)\n",
            "\n",
            "Current learning rate: 0.399999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.24381078779697418 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0608, Accuracy: 9802/10000 (98.02%)\n",
            "\n",
            "Current learning rate: 0.379863\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.225237175822258 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.64it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0560, Accuracy: 9837/10000 (98.37%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0538, Accuracy: 9838/10000 (98.38%)\n",
            "\n",
            "Current learning rate: 0.324103\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.21478641033172607 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0600, Accuracy: 9817/10000 (98.17%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0459, Accuracy: 9863/10000 (98.63%)\n",
            "\n",
            "Current learning rate: 0.243764\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.15717071294784546 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0400, Accuracy: 9870/10000 (98.70%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0425, Accuracy: 9862/10000 (98.62%)\n",
            "\n",
            "Current learning rate: 0.154757\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.2991573214530945 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.67it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0398, Accuracy: 9880/10000 (98.80%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0391, Accuracy: 9884/10000 (98.84%)\n",
            "\n",
            "Current learning rate: 0.074711\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.08748733252286911 batch_id=117: 100%|██████████| 118/118 [00:21<00:00,  5.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0412, Accuracy: 9879/10000 (98.79%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0417, Accuracy: 9864/10000 (98.64%)\n",
            "\n",
            "Current learning rate: 0.019481\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.1466939002275467 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.63it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0412, Accuracy: 9870/10000 (98.70%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0406, Accuracy: 9880/10000 (98.80%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = Net().to(device)\n",
        "from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "\n",
        "swa_model = AveragedModel(model)\n",
        "\n",
        "# Use OneCycleLR scheduler for better convergence\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.4,\n",
        "    epochs=10,                          # Increased epochs\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=0.3,                      # Warm up for 30% of training\n",
        "    div_factor=10,                      # Initial lr = max_lr/10\n",
        "    final_div_factor=10000                # Final lr = initial_lr/100\n",
        ")\n",
        "\n",
        "# Start SWA later in training\n",
        "swa_start = 5\n",
        "swa_scheduler = SWALR(optimizer, swa_lr=0.001)\n",
        "\n",
        "for epoch in range(1, 11):  # Increased to 20 epochs\n",
        "    # Get current learning rate\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f'Current learning rate: {current_lr:.6f}')\n",
        "    \n",
        "    train(model, device, train_loader, optimizer, epoch, scheduler)\n",
        "    if epoch >= swa_start:\n",
        "        swa_model.update_parameters(model)\n",
        "        #swa_scheduler.step()\n",
        "    \n",
        "    test(model, device, test_loader)\n",
        "    \n",
        "    if epoch >= swa_start:\n",
        "        torch.optim.swa_utils.update_bn(train_loader, swa_model, device=device)\n",
        "        test(swa_model, device, test_loader)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current learning rate: 0.008000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.44018951058387756 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1245, Accuracy: 9621/10000 (96.21%)\n",
            "\n",
            "Current learning rate: 0.030541\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.32167598605155945 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0738, Accuracy: 9780/10000 (97.80%)\n",
            "\n",
            "Current learning rate: 0.087579\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.31309664249420166 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0822, Accuracy: 9749/10000 (97.49%)\n",
            "\n",
            "Current learning rate: 0.152328\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.10911029577255249 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0578, Accuracy: 9802/10000 (98.02%)\n",
            "\n",
            "Current learning rate: 0.194382\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.19831985235214233 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0693, Accuracy: 9787/10000 (97.87%)\n",
            "\n",
            "Current learning rate: 0.198845\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.3008888065814972 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.66it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0596, Accuracy: 9797/10000 (97.97%)\n",
            "\n",
            "Current learning rate: 0.189987\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.22673659026622772 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0403, Accuracy: 9868/10000 (98.68%)\n",
            "\n",
            "Current learning rate: 0.173133\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.20961563289165497 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0390, Accuracy: 9873/10000 (98.73%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0386, Accuracy: 9874/10000 (98.74%)\n",
            "\n",
            "Current learning rate: 0.000100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.30503180623054504 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0392, Accuracy: 9880/10000 (98.80%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0365, Accuracy: 9881/10000 (98.81%)\n",
            "\n",
            "Current learning rate: 0.000100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.15647047758102417 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0394, Accuracy: 9874/10000 (98.74%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0391, Accuracy: 9882/10000 (98.82%)\n",
            "\n",
            "Current learning rate: 0.000100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.15107975900173187 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0383, Accuracy: 9873/10000 (98.73%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0407, Accuracy: 9871/10000 (98.71%)\n",
            "\n",
            "Current learning rate: 0.000100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.12172702699899673 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0387, Accuracy: 9876/10000 (98.76%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0386, Accuracy: 9886/10000 (98.86%)\n",
            "\n",
            "Current learning rate: 0.000100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.056091148406267166 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0390, Accuracy: 9867/10000 (98.67%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0387, Accuracy: 9879/10000 (98.79%)\n",
            "\n",
            "Current learning rate: 0.000100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.1886254996061325 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.70it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0386, Accuracy: 9874/10000 (98.74%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0384, Accuracy: 9873/10000 (98.73%)\n",
            "\n",
            "Current learning rate: 0.000100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.13730067014694214 batch_id=117: 100%|██████████| 118/118 [00:20<00:00,  5.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0368, Accuracy: 9881/10000 (98.81%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0389, Accuracy: 9881/10000 (98.81%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = Net().to(device)\n",
        "swa_model = AveragedModel(model)\n",
        "\n",
        "# Modified optimizer and scheduler settings\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.2,                         # Reduced max_lr for more stability\n",
        "    epochs=15,                          # Increased epochs\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=0.3,                      \n",
        "    div_factor=25,                      # Initial lr = max_lr/25 = 0.008\n",
        "    final_div_factor=1e4                # Final lr = initial_lr/10000 ≈ 8e-7\n",
        ")\n",
        "\n",
        "# Start SWA later in training\n",
        "swa_start = 8                           # Start SWA later to let OneCycleLR work longer\n",
        "swa_scheduler = SWALR(optimizer, swa_lr=0.0001)  # Reduced SWA learning rate\n",
        "\n",
        "for epoch in range(1, 16):\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f'Current learning rate: {current_lr:.6f}')\n",
        "    \n",
        "    if epoch < swa_start:\n",
        "        train(model, device, train_loader, optimizer, epoch, scheduler)\n",
        "    else:\n",
        "        # After SWA starts, use a very small constant learning rate\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = 0.001\n",
        "        train(model, device, train_loader, optimizer, epoch)\n",
        "        swa_model.update_parameters(model)\n",
        "    \n",
        "    test(model, device, test_loader)\n",
        "    \n",
        "    if epoch >= swa_start:\n",
        "        torch.optim.swa_utils.update_bn(train_loader, swa_model, device=device)\n",
        "        test(swa_model, device, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
