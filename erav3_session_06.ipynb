{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h_Cx9q2QFgM7"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, 3, padding=1),  # 1*3*3*4 + 4 = 40 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(4),              # 8 params\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(4, 8, 3, padding=1),  # 4*3*3*8 + 8 = 296 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),              # 16 params\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 12, 3, padding=1), # 8*3*3*12 + 12 = 876 params\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(12),             # 24 params\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        \n",
        "        # 12 channels * 3 * 3 = 108 neurons after three max pools (28->14->7->3)\n",
        "        self.fc1 = nn.Linear(12 * 3 * 3, 10)  # 108*10 + 10 = 1090 params\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1, 12 * 3 * 3)  # Flatten\n",
        "        x = F.dropout(x, p=0.1)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xdydjYTZFyi3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Net                                      [1, 10]                   --\n",
              "├─Sequential: 1-1                        [1, 4, 14, 14]            --\n",
              "│    └─Conv2d: 2-1                       [1, 4, 28, 28]            40\n",
              "│    └─ReLU: 2-2                         [1, 4, 28, 28]            --\n",
              "│    └─BatchNorm2d: 2-3                  [1, 4, 28, 28]            8\n",
              "│    └─MaxPool2d: 2-4                    [1, 4, 14, 14]            --\n",
              "├─Sequential: 1-2                        [1, 8, 7, 7]              --\n",
              "│    └─Conv2d: 2-5                       [1, 8, 14, 14]            296\n",
              "│    └─ReLU: 2-6                         [1, 8, 14, 14]            --\n",
              "│    └─BatchNorm2d: 2-7                  [1, 8, 14, 14]            16\n",
              "│    └─MaxPool2d: 2-8                    [1, 8, 7, 7]              --\n",
              "│    └─Dropout: 2-9                      [1, 8, 7, 7]              --\n",
              "├─Sequential: 1-3                        [1, 12, 3, 3]             --\n",
              "│    └─Conv2d: 2-10                      [1, 12, 7, 7]             876\n",
              "│    └─ReLU: 2-11                        [1, 12, 7, 7]             --\n",
              "│    └─BatchNorm2d: 2-12                 [1, 12, 7, 7]             24\n",
              "│    └─MaxPool2d: 2-13                   [1, 12, 3, 3]             --\n",
              "│    └─Dropout: 2-14                     [1, 12, 3, 3]             --\n",
              "├─Linear: 1-4                            [1, 10]                   1,090\n",
              "==========================================================================================\n",
              "Total params: 2,350\n",
              "Trainable params: 2,350\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.13\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.08\n",
              "Params size (MB): 0.01\n",
              "Estimated Total Size (MB): 0.10\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "# Create a dummy input tensor on the correct device\n",
        "summary(model, input_size=(1, 1, 28, 28), device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/dhairyashil/miniconda3/envs/erav2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/Users/dhairyashil/miniconda3/envs/erav2/lib/python3.12/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "torch.manual_seed(1456)\n",
        "batch_size = 512\n",
        "\n",
        "kwargs = {'num_workers': 0, 'pin_memory': True} if device.type in [\"cuda\", \"mps\"] else {}\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=True, download=True,\n",
        "#                     transform=transforms.Compose([\n",
        "#                         transforms.ToTensor(),\n",
        "#                         transforms.Normalize((0.1307,), (0.3081,))\n",
        "#                     ])),\n",
        "#     batch_size=batch_size, shuffle=True, **kwargs)\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "#                         transforms.ToTensor(),\n",
        "#                         transforms.Normalize((0.1307,), (0.3081,))\n",
        "#                     ])),\n",
        "#     batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "# Define the augmentation pipeline\n",
        "# train_transforms = A.Compose([\n",
        "#     A.ShiftScaleRotate(\n",
        "#         shift_limit=0.0625,\n",
        "#         scale_limit=0.1,\n",
        "#         rotate_limit=15,\n",
        "#         p=0.7,\n",
        "#         border_mode=cv2.BORDER_CONSTANT,\n",
        "#         value=0\n",
        "#     ),\n",
        "#     A.GridDistortion(num_steps=5, distort_limit=0.2, p=0.3),\n",
        "#     A.GaussNoise(var_limit=(5.0, 30.0), p=0.3),\n",
        "#     A.Perspective(scale=(0.05, 0.1), p=0.3, keep_size=True, pad_mode=cv2.BORDER_CONSTANT, pad_val=0),\n",
        "#     A.Normalize(\n",
        "#         mean=[0.1307],\n",
        "#         std=[0.3081],\n",
        "#     ),\n",
        "#     ToTensorV2(),\n",
        "# ])\n",
        "\n",
        "train_transforms = A.Compose([\n",
        "    # Essential transforms for MNIST\n",
        "    A.ShiftScaleRotate(\n",
        "        shift_limit=0.0625,      # Increased slightly\n",
        "        scale_limit=0.1,     # Increased slightly\n",
        "        rotate_limit=15,      # Increased rotation range\n",
        "        p=0.8,               # Increased probability\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        value=0\n",
        "    ),\n",
        "    \n",
        "    # Remove GridDistortion as it might be too aggressive for MNIST\n",
        "    \n",
        "    # Modified noise parameters\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
        "    \n",
        "    # Added more relevant transforms\n",
        "    A.RandomBrightnessContrast(\n",
        "        brightness_limit=0.2,\n",
        "        contrast_limit=0.2,\n",
        "        p=0.2\n",
        "    ),\n",
        "    \n",
        "    # Correct parameters for ElasticTransform\n",
        "    A.ElasticTransform(\n",
        "        alpha=1.0,\n",
        "        sigma=10.0,\n",
        "        alpha_affine=None,  # Set to None as required by newer versions\n",
        "        interpolation=cv2.INTER_LINEAR,\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        value=0,\n",
        "        p=0.3\n",
        "    ),\n",
        "    \n",
        "    A.Normalize(\n",
        "        mean=[0.1307],\n",
        "        std=[0.3081],\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# Custom Dataset class to work with Albumentations\n",
        "class MNISTAlbumentations(datasets.MNIST):\n",
        "    def __init__(self, root, train=True, download=True, transform=None):\n",
        "        super().__init__(root, train=train, download=download, transform=None)\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.data[idx], self.targets[idx]\n",
        "        \n",
        "        # Convert to numpy array and add channel dimension\n",
        "        img = np.array(img)\n",
        "        img = np.expand_dims(img, axis=-1)  # Add channel dimension for Albumentations\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            transformed = self.transform(image=img)\n",
        "            img = transformed[\"image\"]\n",
        "            \n",
        "        return img, label\n",
        "\n",
        "\n",
        "# Update the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    MNISTAlbumentations('../data', train=True, download=True, transform=train_transforms),\n",
        "    batch_size=batch_size, shuffle=True, \n",
        "    **kwargs)\n",
        "\n",
        "# Test transforms (only normalization, no augmentation)\n",
        "test_transforms = A.Compose([\n",
        "    A.Normalize(\n",
        "        mean=[0.1307],\n",
        "        std=[0.3081],\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    MNISTAlbumentations('../data', train=False, transform=test_transforms),\n",
        "    batch_size=batch_size, shuffle=True,  \n",
        "    **kwargs)\n",
        "\n",
        "# Optional: Visualization function to check augmentations\n",
        "def visualize_augmentations(dataset, idx=0, samples=5):\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for i in range(samples):\n",
        "        data = dataset[idx][0]\n",
        "        if isinstance(data, torch.Tensor):\n",
        "            data = data.numpy()\n",
        "        if data.shape[0] == 1:  # If channels first, move to last\n",
        "            data = np.transpose(data, (1, 2, 0))\n",
        "        plt.subplot(1, samples, i + 1)\n",
        "        plt.imshow(data.squeeze(), cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x400 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples in train dataset: 60000\n",
            "Number of samples in test dataset: 10000\n"
          ]
        }
      ],
      "source": [
        "# Uncomment to visualize augmentations\n",
        "visualize_augmentations(train_loader.dataset)\n",
        "\n",
        "# # print number of samples in train and test dataset\n",
        "print(f\"Number of samples in train dataset: {len(train_loader.dataset)}\")\n",
        "print(f\"Number of samples in test dataset: {len(test_loader.dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MMWbLWO6FuHb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.5302174687385559 batch_id=117: 100%|██████████| 118/118 [00:11<00:00, 10.37it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1768, Accuracy: 9466/10000 (94.66%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.19213242828845978 batch_id=117: 100%|██████████| 118/118 [00:10<00:00, 11.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1141, Accuracy: 9661/10000 (96.61%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.19363141059875488 batch_id=117: 100%|██████████| 118/118 [00:10<00:00, 10.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0905, Accuracy: 9729/10000 (97.29%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.19611263275146484 batch_id=117: 100%|██████████| 118/118 [00:10<00:00, 11.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0805, Accuracy: 9747/10000 (97.47%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.20056571066379547 batch_id=117: 100%|██████████| 118/118 [00:10<00:00, 11.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0714, Accuracy: 9756/10000 (97.56%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.10945314168930054 batch_id=117: 100%|██████████| 118/118 [00:10<00:00, 10.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0698, Accuracy: 9764/10000 (97.64%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.15322934091091156 batch_id=117: 100%|██████████| 118/118 [00:10<00:00, 10.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0641, Accuracy: 9807/10000 (98.07%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.11314129829406738 batch_id=117: 100%|██████████| 118/118 [00:11<00:00, 10.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0673, Accuracy: 9793/10000 (97.93%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loss=0.13212358951568604 batch_id=117: 100%|██████████| 118/118 [00:10<00:00, 10.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0646, Accuracy: 9787/10000 (97.87%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
        "# scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "#     optimizer, \n",
        "#     max_lr=0.1,\n",
        "#     epochs=10,\n",
        "#     steps_per_epoch=len(train_loader)\n",
        "# )\n",
        "#optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
        "\n",
        "for epoch in range(1, 10):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So5uk4EkHW6R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
